<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="你好，世界！"><title>词向量系列（3）：深入Word2Vec及其实现 | Erwin Feng Blog</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">词向量系列（3）：深入Word2Vec及其实现</h1><a id="logo" href="/blog/.">Erwin Feng Blog</a><p class="description">内容也是一种社交方式！</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a><a href="/blog/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">词向量系列（3）：深入Word2Vec及其实现</h1><div class="post-meta">2019-02-12<span> | </span><span class="category"><a href="/blog/categories/NLP/">NLP</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#NNLM"><span class="toc-number">1.</span> <span class="toc-text">NNLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E8%AF%B4%E8%AF%B4Embedding"><span class="toc-number">2.</span> <span class="toc-text">简单说说Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word2vec"><span class="toc-number">3.</span> <span class="toc-text">word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CBOW-%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">CBOW 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-gram"><span class="toc-number">5.</span> <span class="toc-text">skip-gram</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">训练与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">6.1.</span> <span class="toc-text">负采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B1%82-softmax"><span class="toc-number">6.2.</span> <span class="toc-text">分层 softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E9%87%87%E6%A0%B7"><span class="toc-number">6.3.</span> <span class="toc-text">降采样</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E8%A1%A5%E5%85%85%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">实现与应用（补充）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">9.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="post-content"><p>更新了Word2Vec的实现，包括若干技巧。</p>
<span id="more"></span>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1402.3722.pdf">word2vec</a> 基于局部上下文窗口的方法。</p>
<h2 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h2><p>可以说，<a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> 这篇 paper 是词的分布式表示的开篇鼻祖，不过当时，NNLM 模型关注的是语言模型，具体是一个n-grams的语言模型，</p>
<script type="math/tex; mode=display">
p(w_{i} | w_{i-n+1},\dots,w_{i-1})</script><p>模型训练好后，有一个副产品即词向量，也就是Embedding层（第一层参数）。Embedding层输出后的词向量序列直接拼接起来组成一个更大的向量。接在做更多的处理后送入softmax预测目标词。</p>
<p>NNLM本质是一个语言模型，但训练好后有一个副产品就是词向量，也就是第一层参数，用现在的深度学习框架来说就是Embedding。</p>
<p>NNLM的做法直接启发了词的向量表示，接下来简单说说Embedding是什么。</p>
<h2 id="简单说说Embedding"><a href="#简单说说Embedding" class="headerlink" title="简单说说Embedding"></a>简单说说Embedding</h2><p>Embedding是一个没有激活函数和偏置项的Dense网络，也就是一个矩阵$\boldsymbol{W} \in \mathbb{R}^{|V| \times m}$​，对于词$w_i$表示词表中的第$i$个词，用one-hot表示有，</p>
<script type="math/tex; mode=display">
\operatorname{one-hot}(w_i) = [0, \dots, 1, \dots, 0]</script><p>位置$i$为1，其他为$0$​。Embedding要做的事情就是把词$w_i$的onehot离散表示转变为稠密的向量，</p>
<script type="math/tex; mode=display">
\begin{align}
\boldsymbol{x}_i 
&= \operatorname{one-hot}(w_i) \times \boldsymbol{W} \newline
&= [0, \dots, 1, \dots, 0] \times \boldsymbol{W} \newline
&=[0, \dots, 1, \dots, 0] \times \begin{bmatrix}
w_{1,1} & w_{1,2} & \dots & w_{1,m} \cr
w_{2,1} & w_{2,2} & \dots & w_{2,m} \cr
\vdots & \vdots & \vdots & \vdots \cr
w_{n,1} & w_{n,2} & \dots & w_{n,m}
\end{bmatrix} \newline
&= [w_{i1}, w_{i2}, \dots, w_{im}]
\end{align}</script><p>也就相当于取矩阵$\boldsymbol{W}$的第$i$行作为词$w_i$的稠密表示。这意味着，$\operatorname{one-hot}(w_i)$​与矩阵$\boldsymbol{W}$相乘直接改矩阵的第$i$行即可，不需要繁复的矩阵乘积。</p>
<p>这就是Embedding的原理和作用，可以简单表示为，</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_i = \operatorname{Embedding}([i])</script><p>词$w_i$在字表V中对应编码$i$输入Embedding，获得该词的稠密表示。在实践中，对于OOV的词，可以使用一个固定的编码表示，如<code>&lt;UNK&gt;=1</code>。于是，对于句子（词ID序列）$i,j,k,\dots$，有</p>
<script type="math/tex; mode=display">
[\boldsymbol{x}_i , \boldsymbol{x}_j , \boldsymbol{x}_k , \dots] = \operatorname{Embedding}([i, j, k, \dots])</script><p>有了这种简单的表示工具后，CBOW模型就很容易表达了。</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p> CBOW 和 skip-gram 都是 word2vec 的实现方法，两者的网络架构分别如下，</p>
<p><img src="../images/nn-word2vec.png" alt="nn-word2vec.png"></p>
<p>学习词向量的<strong>无监督方法</strong>大致可以分为两类：</p>
<ul>
<li>基于全局共现矩阵及其分解技巧，如上一篇中的LSA以及其基于SVD</li>
<li><p>基于滑动的上下文窗口</p>
<p>CBOW 和 skip-gram 都是基于滑动的上下文窗口的模型。下面我们分别来介绍这两个模型。</p>
</li>
</ul>
<h2 id="CBOW-模型"><a href="#CBOW-模型" class="headerlink" title="CBOW 模型"></a>CBOW 模型</h2><p>CBOW 根据上下文来预测当前单词，就像时间序列预测中，根据历史序列预测下一个时间步的取值。具体是这样的，假设窗口大小为$C=n$​​的滑动窗口下有词序列$[w_1, w_2, \dots, w_{m-1}, w_m, w_{m+1}, \dots, w_n]$​​，CBOW要做的事情是使用$[w_1, w_2, \dots, w_{m-1}, w_{m+1}, \dots, w_n]$​​作为<strong>上下文</strong>去预测词$w_m$​​，即</p>
<script type="math/tex; mode=display">
w_m = f\Big([w_1, w_2, \dots, w_{m-1}, w_{m+1}, \dots, w_n]\Big)</script><p>例如两个词窗口大小$C=2$的特例，</p>
<script type="math/tex; mode=display">
w_m = f([w_{m-1}, w_{m+1}])</script><p>对于一个词的上下文，窗口大小为$C=1$，有特例</p>
<script type="math/tex; mode=display">
w_m = f([w_{m-1}])</script><p>即用当前词预测下一个词。</p>
<p>CBOW模型是一个带单隐层的前馈神经网络，也就是Embedding层，其上下文向量是窗口内词的Embedding输出的均值。这里依旧假设窗口大小为$C=n$​​，那么CBOW的上下文向量为，</p>
<script type="math/tex; mode=display">
\boldsymbol{h} = \frac{1}{C}\sum_{i=1}^{n} \boldsymbol{x}_{i}</script><p>对于特例$C=1$​，就是使用当前单词预测下一个单词，于是$\boldsymbol{h} = \boldsymbol{x}_{n-1}$。</p>
<p>CBOW模型模型的输出是对，</p>
<script type="math/tex; mode=display">
P(w_m | \boldsymbol{h})</script><p>建模，即用上下文向量预测当前词。具体是，</p>
<script type="math/tex; mode=display">
\boldsymbol{y} = \operatorname{softmax}(\boldsymbol{h} \times \boldsymbol{W}')</script><p>为词表V的概率分布，其中$\boldsymbol{W}’ \in \mathbb{R}^{m \times |V|}$。根据交叉式损失函数，</p>
<script type="math/tex; mode=display">
L = -\boldsymbol{w}'_m \cdot \boldsymbol{h} + \log (\sum_{i=1}^{V} e^{\boldsymbol{w}'_i \cdot \boldsymbol{h}})</script><p>$\boldsymbol{w}’_i$为矩阵$\boldsymbol{W}’ \in \mathbb{R}^{m \times |V|}$的第$i$列。于是有总体目标，</p>
<script type="math/tex; mode=display">
\min L = \min_{\boldsymbol{W}, \boldsymbol{W}'} \sum_{\big([w_1, w_2, \dots, w_k], w_m \big)} \Big[-\boldsymbol{w}'_m \cdot \boldsymbol{h} + \log (\sum_{i=1}^{V} e^{\boldsymbol{w}'_i \cdot \boldsymbol{h}}) \Big]</script><h2 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h2><p>skip-gram 和 CBOW相反，根据给定单词预测其上下文，即</p>
<script type="math/tex; mode=display">
[w_1, w_2, \dots, w_{m-1}, w_{m+1}, \dots, w_n]= f(w_m)</script><p>同样，每个词通过Embedding后获得向量化表示，</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_i = \operatorname{Embedding}(w_i)</script><p>skip-gram的目标函数是<strong>最大化多个输出的联合概率分布</strong>，因此</p>
<script type="math/tex; mode=display">
\begin{align}
L 
&= - \log p(w_1, \dots, w_n| w_m) \newline
&= -\log \prod_{i=1}^{C} \frac{\exp(u_i)}{\displaystyle \sum_{j=1}^{|V|} \exp(u_j)} \newline
&= -\sum_{i=1}^{C} u_i + C \log\left(\sum_{j=1}^{|V|} \exp(u_j) \right)
\end{align}</script><p>相比与CBOW采用上下文来预测当前词，skip-gram则是采用当前词来预测上下文，两种的网络结构都相似，理论上单独训练获得的词向量效果相似。不过在大型语料下，建议采用skip-gram，因为其表现往往更优。</p>
<h2 id="训练与优化"><a href="#训练与优化" class="headerlink" title="训练与优化"></a>训练与优化</h2><p>word2vec直接训练的难度很大，为此提出两种优化策略：</p>
<ul>
<li>分层 softmax</li>
<li>负采样</li>
</ul>
<h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>CBOW要根据上下文预测输出的词，输出可以看做是分类过程，只不过分类类比为词汇表中的所有词汇，因此在计算softmax时涉及到的计算，</p>
<script type="math/tex; mode=display">
\operatorname{softmax}(\boldsymbol{x})_{[i]} = \frac{\exp(x_i)}{\displaystyle \sum_{x_i \in V}\exp(x_i) }, \; i = 1, 2, \dots, |V|</script><p>主要的计算量集中在分母的归一化上。与之对应的是优化中计算，</p>
<script type="math/tex; mode=display">
\log( \sum_{x_i \in V} \exp(x_i))</script><p>计算量非常大。注意到这里的类别涉及两类，要预测的词，称为正类，其他词，称为负类。因此对于softmax输出的类别分布向量中，只有一个位置是表示正类的概率，剩下的$|V|-1$个位置都是负类的概率。为了缩减计算量，再保留正类的情况下，随机采样一批负类来替代整个词汇表的计算是不错的优化方案。于是，从词汇表$V$​​中采样一个子集$R = {w^+, w_1^-, \dots, w_{n-1}^- } \sub V $​​​​，有</p>
<script type="math/tex; mode=display">
\log( \sum_{x_i \in R} \exp(x_i))</script><p>那么如何解决负类如何采样问题？负类样本采样多少个？</p>
<p>负类样本的采样应该根据如下概率分布采样，</p>
<script type="math/tex; mode=display">
P_n(w) = \frac{f(w_i^-) ^{\frac{3}{4}}}{\displaystyle \sum_{w_i^- \ne w_i^+} f(w_i^-) ^{\frac{3}{4}}}</script><p>该分布也称为噪声分布。其中$f(w)$表示词$w$在语料库中的概率。一般建议5~10个负样本即可，这是数量级上减少计算量。</p>
<p>为什么能行？这会不会导致模型偏离原来的学习目标？其实以上思路就是噪音对比估计（NCE，Noise Contrastive Estimation）思想。其基本思想是通过一定的技巧去估计概率分布的参数进而避免复杂的归一化计算。可以参考论文：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1402.3722.pdf">word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method</a></li>
<li><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></li>
</ul>
<h3 id="分层-softmax"><a href="#分层-softmax" class="headerlink" title="分层 softmax"></a>分层 softmax</h3><p>分层softmax本质是对softmax的近似，对于softmax难计算场景，都可以使用分层softmax来近似，它把原来的计算分为$\log(N)$层的二分类，这个现在使用较少，不展开。</p>
<h3 id="降采样"><a href="#降采样" class="headerlink" title="降采样"></a>降采样</h3><p>对于一些常见的词，如停用词，我们应该在语料库中随机删除它，因为对于CBOW来说：</p>
<ul>
<li>停用词出现在上下文时，这类词并只能为目标词提供极少语义上的信息</li>
<li>当这类词作为目标词时，该词没有提供多少语义信息，没有必要频繁学习</li>
</ul>
<p>类似第，skip-gram也适用以上分析。</p>
<p>于是这里提出降采样思路。降采样中，词$w$被采样的概率，</p>
<script type="math/tex; mode=display">
p(w) = \frac{r}{z(w)} \times \left(\sqrt{\frac{z(w)}{r}} + 1\right)</script><p>其中$r = 0.001$​为降采样率，等号右侧是推荐的默认值。$z(w)$​是$w$在语料库中出现的概率。该采样思路整体意义上是要求，单词在语料库中出现的词频越大，保留该单词的概率越小。</p>
<p>降采样方法是配合窗口窗口生成训练样本使用，其具体实现可参看源码。</p>
<h2 id="实现与应用（补充）"><a href="#实现与应用（补充）" class="headerlink" title="实现与应用（补充）"></a>实现与应用（补充）</h2><p>先前都是使用gensim来使用word2vec，这里补充一下基于 tensorflow2.x 实现的 word2vec：<a target="_blank" rel="noopener" href="https://github.com/allenwind/word2vec-in-tensorflow2.0，包括两个模型（CBOW和SkipGram）和两种优化方案（Hierarchical">https://github.com/allenwind/word2vec-in-tensorflow2.0，包括两个模型（CBOW和SkipGram）和两种优化方案（Hierarchical</a> softmax和负采样），这里开源负采样、CBOW和SkipGram的实现。</p>
<p>以CBOW为例，模型训练（需要训练语料，可自行调整）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python model_cbow.py</span><br></pre></td></tr></table></figure>
<p>模型训练好后简单的相似计算：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python tools.py</span><br></pre></td></tr></table></figure>
<p>简单的交互测试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from tools import topk_similar</span><br><span class="line">&gt;&gt;&gt; import pprint</span><br><span class="line">&gt;&gt;&gt; pprint.pprint(topk_similar(<span class="string">&quot;数学&quot;</span>))</span><br><span class="line">[(<span class="string">&#x27;数学&#x27;</span>, 1.0000001),</span><br><span class="line"> (<span class="string">&#x27;语文&#x27;</span>, 0.9285573),</span><br><span class="line"> (<span class="string">&#x27;试题&#x27;</span>, 0.9178058),</span><br><span class="line"> (<span class="string">&#x27;英语&#x27;</span>, 0.91363966),</span><br><span class="line"> (<span class="string">&#x27;专业课&#x27;</span>, 0.9079031),</span><br><span class="line"> (<span class="string">&#x27;考&#x27;</span>, 0.9042375),</span><br><span class="line"> (<span class="string">&#x27;高等数学&#x27;</span>, 0.897079),</span><br><span class="line"> (<span class="string">&#x27;分值&#x27;</span>, 0.89574695),</span><br><span class="line"> (<span class="string">&#x27;雅思&#x27;</span>, 0.893947),</span><br><span class="line"> (<span class="string">&#x27;外语&#x27;</span>, 0.88838947)]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pprint.pprint(topk_similar(<span class="string">&quot;资本&quot;</span>))</span><br><span class="line">[(<span class="string">&#x27;资本&#x27;</span>, 1.0),</span><br><span class="line"> (<span class="string">&#x27;外资&#x27;</span>, 0.84800375),</span><br><span class="line"> (<span class="string">&#x27;融资&#x27;</span>, 0.8301372),</span><br><span class="line"> (<span class="string">&#x27;资产&#x27;</span>, 0.81580234),</span><br><span class="line"> (<span class="string">&#x27;投资人&#x27;</span>, 0.78543866),</span><br><span class="line"> (<span class="string">&#x27;投资&#x27;</span>, 0.7799219),</span><br><span class="line"> (<span class="string">&#x27;并购&#x27;</span>, 0.77950025),</span><br><span class="line"> (<span class="string">&#x27;承销&#x27;</span>, 0.7720045),</span><br><span class="line"> (<span class="string">&#x27;发债&#x27;</span>, 0.76935923),</span><br><span class="line"> (<span class="string">&#x27;管理层&#x27;</span>, 0.7664337)]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; pprint.pprint(topk_similar(<span class="string">&quot;CPU&quot;</span>))</span><br><span class="line">[(<span class="string">&#x27;CPU&#x27;</span>, 1.0),</span><br><span class="line"> (<span class="string">&#x27;GPU&#x27;</span>, 0.9385017),</span><br><span class="line"> (<span class="string">&#x27;1GHz&#x27;</span>, 0.9292523),</span><br><span class="line"> (<span class="string">&#x27;功耗&#x27;</span>, 0.9110091),</span><br><span class="line"> (<span class="string">&#x27;ROM&#x27;</span>, 0.90495074),</span><br><span class="line"> (<span class="string">&#x27;2G&#x27;</span>, 0.904893),</span><br><span class="line"> (<span class="string">&#x27;芯片&#x27;</span>, 0.90474397),</span><br><span class="line"> (<span class="string">&#x27;芯&#x27;</span>, 0.90301657),</span><br><span class="line"> (<span class="string">&#x27;低功耗&#x27;</span>, 0.8979711),</span><br><span class="line"> (<span class="string">&#x27;256MB&#x27;</span>, 0.89573085)]</span><br></pre></td></tr></table></figure>
<p>一个case，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = m.most_similar(<span class="string">&quot;算法&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pprint.pprint(s)</span><br><span class="line">[(<span class="string">&#x27;遗传算法&#x27;</span>, <span class="number">0.8080635070800781</span>),</span><br><span class="line"> (<span class="string">&#x27;神经网络&#x27;</span>, <span class="number">0.775327205657959</span>),</span><br><span class="line"> (<span class="string">&#x27;控制算法&#x27;</span>, <span class="number">0.7712478637695312</span>),</span><br><span class="line"> (<span class="string">&#x27;搜索算法&#x27;</span>, <span class="number">0.7440242767333984</span>),</span><br><span class="line"> (<span class="string">&#x27;智能算法&#x27;</span>, <span class="number">0.7334689497947693</span>),</span><br><span class="line"> (<span class="string">&#x27;递归&#x27;</span>, <span class="number">0.7318022847175598</span>),</span><br><span class="line"> (<span class="string">&#x27;模拟退火&#x27;</span>, <span class="number">0.7306191921234131</span>),</span><br><span class="line"> (<span class="string">&#x27;迭代&#x27;</span>, <span class="number">0.7223993539810181</span>),</span><br><span class="line"> (<span class="string">&#x27;求解&#x27;</span>, <span class="number">0.7199177145957947</span>),</span><br><span class="line"> (<span class="string">&#x27;鲁棒性&#x27;</span>, <span class="number">0.712174654006958</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure>
<p>“算法”和“鲁棒性”并不相似，它们更像是共现。</p>
<p>以上的训练直接使用<a target="_blank" rel="noopener" href="http://thuctc.thunlp.org/">THUCNews语料</a>。</p>
<p>源码地址：<a target="_blank" rel="noopener" href="https://github.com/allenwind/word2vec-in-tensorflow2.0">https://github.com/allenwind/word2vec-in-tensorflow2.0</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>word2vec 转为词向量涉及，里面有大量的技巧提高训练效率，而不同于语言模型，后者词向量是副产品。因此，word2vec获得的词向量更具通用性。</p>
<p>word2vec中CBOW对比skip-gram：</p>
<ul>
<li>CBOW用周围词预测中心词；skip-gram用中心词预测周围词</li>
<li>skip-gram用中心词预测周围词，做的预测比CBOW多，训练时间更长</li>
<li>CBOW训练效率更高，skip-gram训练效率慢一点，但是通过预测周围词来训练中心词的向量表示，表示能力往往更好</li>
<li>因此，CBOW与skip-gram有效率与性能上的取舍。</li>
</ul>
<p>word2vec优缺点：</p>
<ul>
<li>优点：在单词类比任务中表现较好，即king - man = queen - woman这类例子。这说明模型具有词汇理解能力，想象一下我们做的类比推理。</li>
<li>缺点：因为word2vec在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息。</li>
<li>缺点：对词是确定性的，即在不同语境中同一词的向量是一样的，这意味着 word2vec 不能解决多义词问题。</li>
</ul>
<p>word2vec一再被人证明是一个全局PPMI（共现）矩阵的分解，参考论文：</p>
<ul>
<li>Neural word embedding as implicit matrix factorization</li>
<li>Word embedding revisited: A new representation learning and explicit matrix factorization perspective</li>
</ul>
<p>word2vec和LSA的对比：</p>
<ul>
<li><p>两者的预料都不带标注。</p>
</li>
<li><p>LSA 通过 VSM 获得矩阵并分解得到词向量与主题向量，本质上是无监督过程。</p>
</li>
<li><p>word2vec 是有监督，更准确地说是自监督</p>
</li>
<li><p>LSA 中的 SVD 分解是无监督，而 word2vec 是有监督的。</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Efficient Estimation of Word Representations in Vector Space</p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1402.3722.pdf">word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method</a></p>
<p>[3] Distributed Representations of Words and Phrases and their Compositionality</p>
<p>[4] <a target="_blank" rel="noopener" href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></p>
<p>转载请包括本文地址：<a href="../9163">https://allenwind.github.io/blog/9163</a><br>更多文章请参考：<a href="../archives/">https://allenwind.github.io/blog/archives/</a></p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="tag">词向量</a></li></ul></div><div class="post-nav"><a class="pre" href="/blog/9194/">词向量系列（4）：Glove一个别致的思路</a><a class="next" href="/blog/9001/">优化算法系列（5）：SGD改进之Adam一个综合方案和派生（更新）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://allenwind.github.io/blog"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/blog/icon.png"/></a><p>Erwin Feng, 你的认知合作伙伴！</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/allenwind" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/C-C/">C/C++</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E6%95%B0%E5%AD%A6/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/">数据结构和算法</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">机器学习深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/%E8%AE%B0%E5%BD%95/">记录</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/blog/tags/C/" style="font-size: 15px;">C</a> <a href="/blog/tags/Python/" style="font-size: 15px;">Python</a> <a href="/blog/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/blog/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 15px;">数据库</a> <a href="/blog/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/blog/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/" style="font-size: 15px;">区块链</a> <a href="/blog/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" style="font-size: 15px;">分布式</a> <a href="/blog/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/blog/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/blog/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/blog/tags/sudo/" style="font-size: 15px;">sudo</a> <a href="/blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/blog/tags/C%E8%AF%AD%E8%A8%80/" style="font-size: 15px;">C语言</a> <a href="/blog/tags/Git/" style="font-size: 15px;">Git</a> <a href="/blog/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/blog/tags/git/" style="font-size: 15px;">git</a> <a href="/blog/tags/gcc/" style="font-size: 15px;">gcc</a> <a href="/blog/tags/Systemd/" style="font-size: 15px;">Systemd</a> <a href="/blog/tags/%E5%91%BD%E4%BB%A4%E8%A1%8C/" style="font-size: 15px;">命令行</a> <a href="/blog/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/blog/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 15px;">正则表达式</a> <a href="/blog/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 15px;">操作系统</a> <a href="/blog/tags/%E8%AE%B0%E5%BD%95/" style="font-size: 15px;">记录</a> <a href="/blog/tags/go/" style="font-size: 15px;">go</a> <a href="/blog/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a> <a href="/blog/tags/Go/" style="font-size: 15px;">Go</a> <a href="/blog/tags/LRU/" style="font-size: 15px;">LRU</a> <a href="/blog/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/blog/tags/Java/" style="font-size: 15px;">Java</a> <a href="/blog/tags/%E5%89%91%E6%8C%87Offer/" style="font-size: 15px;">剑指Offer</a> <a href="/blog/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/" style="font-size: 15px;">搜索引擎</a> <a href="/blog/tags/%E5%87%BD%E6%95%B0%E5%BC%8F/" style="font-size: 15px;">函数式</a> <a href="/blog/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">网络</a> <a href="/blog/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">并发编程</a> <a href="/blog/tags/Stack/" style="font-size: 15px;">Stack</a> <a href="/blog/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/blog/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">并发</a> <a href="/blog/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" style="font-size: 15px;">网络编程</a> <a href="/blog/tags/%E5%BA%A6%E9%87%8F/" style="font-size: 15px;">度量</a> <a href="/blog/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 15px;">数学</a> <a href="/blog/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/blog/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/" style="font-size: 15px;">时间序列</a> <a href="/blog/tags/%E6%8A%95%E8%B5%84/" style="font-size: 15px;">投资</a> <a href="/blog/tags/%E8%A3%85%E9%A5%B0%E5%99%A8/" style="font-size: 15px;">装饰器</a> <a href="/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/blog/tags/%E7%BB%9F%E8%AE%A1/" style="font-size: 15px;">统计</a> <a href="/blog/tags/%E6%A6%82%E7%8E%87/" style="font-size: 15px;">概率</a> <a href="/blog/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 15px;">贝叶斯</a> <a href="/blog/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" style="font-size: 15px;">最小二乘法</a> <a href="/blog/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">矩阵</a> <a href="/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">深度学习</a> <a href="/blog/tags/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/" style="font-size: 15px;">变分推断</a> <a href="/blog/tags/%E9%9A%8F%E6%9C%BA/" style="font-size: 15px;">随机</a> <a href="/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" style="font-size: 15px;">信息论</a> <a href="/blog/tags/%E5%88%86%E5%B8%83/" style="font-size: 15px;">分布</a> <a href="/blog/tags/%E9%87%87%E6%A0%B7/" style="font-size: 15px;">采样</a> <a href="/blog/tags/%E5%85%89%E6%BB%91/" style="font-size: 15px;">光滑</a> <a href="/blog/tags/%E4%B8%8D%E7%AD%89%E5%BC%8F/" style="font-size: 15px;">不等式</a> <a href="/blog/tags/%E9%80%BC%E8%BF%91/" style="font-size: 15px;">逼近</a> <a href="/blog/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" style="font-size: 15px;">特征工程</a> <a href="/blog/tags/%E9%87%8F%E5%8C%96/" style="font-size: 15px;">量化</a> <a href="/blog/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" style="font-size: 15px;">词向量</a> <a href="/blog/tags/%E4%BF%A1%E6%81%AF/" style="font-size: 15px;">信息</a> <a href="/blog/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">集成学习</a> <a href="/blog/tags/%E5%88%86%E7%B1%BB/" style="font-size: 15px;">分类</a> <a href="/blog/tags/%E8%AF%81%E6%98%8E/" style="font-size: 15px;">证明</a> <a href="/blog/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 15px;">损失函数</a> <a href="/blog/tags/%E5%9B%9E%E5%BD%92/" style="font-size: 15px;">回归</a> <a href="/blog/tags/%E6%8C%87%E6%A0%87/" style="font-size: 15px;">指标</a> <a href="/blog/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" style="font-size: 15px;">交叉验证</a> <a href="/blog/tags/%E6%AD%A3%E5%88%99%E5%8C%96/" style="font-size: 15px;">正则化</a> <a href="/blog/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">神经网络</a> <a href="/blog/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/blog/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/blog/tags/Transformer/" style="font-size: 15px;">Transformer</a> <a href="/blog/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/blog/tags/%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">对抗训练</a> <a href="/blog/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" style="font-size: 15px;">文本分类</a> <a href="/blog/tags/NER/" style="font-size: 15px;">NER</a> <a href="/blog/tags/%E6%A6%82%E7%8E%87%E5%9B%BE/" style="font-size: 15px;">概率图</a> <a href="/blog/tags/%E5%88%86%E8%AF%8D/" style="font-size: 15px;">分词</a> <a href="/blog/tags/%E5%B9%B6%E8%A1%8C/" style="font-size: 15px;">并行</a> <a href="/blog/tags/%E5%9B%BE/" style="font-size: 15px;">图</a> <a href="/blog/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/blog/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size: 15px;">可视化</a> <a href="/blog/tags/%E6%A2%AF%E5%BA%A6/" style="font-size: 15px;">梯度</a> <a href="/blog/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/blog/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/blog/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/" style="font-size: 15px;">注意力</a> <a href="/blog/tags/%E5%BA%8F%E5%88%97%E7%BC%96%E7%A0%81/" style="font-size: 15px;">序列编码</a> <a href="/blog/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/blog/tags/%E5%8D%B7%E7%A7%AF/" style="font-size: 15px;">卷积</a> <a href="/blog/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" style="font-size: 15px;">注意力机制</a> <a href="/blog/tags/%E7%A3%81%E7%9B%98%E6%95%85%E9%9A%9C/" style="font-size: 15px;">磁盘故障</a> <a href="/blog/tags/%E4%BC%98%E5%8C%96/" style="font-size: 15px;">优化</a> <a href="/blog/tags/%E8%B0%83%E5%8F%82/" style="font-size: 15px;">调参</a> <a href="/blog/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/blog/tags/Flask/" style="font-size: 15px;">Flask</a> <a href="/blog/tags/web/" style="font-size: 15px;">web</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/23811/">确定性变量的随机化技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16228/">分析与拓展：Transformer中的MultiHeadAttention为什么使用scaled？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16113/">Tensorflow的多卡训练：原理和实践</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16051/">机器学习之分类问题的评估指标总结</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16003/">天马行空：设计自己的激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15887/">函数光滑近似（4）：Heaviside step函数及其应用</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15205/">引入参数控制softmax的smooth程度</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15110/">分析与拓展：多分类模型的输出为什么使用softmax？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14990/">一种基于光滑逼近的正态分布采样法</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14721/">GELU由来：从狄拉克函数到GELU激活函数</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/blog/." rel="nofollow">Erwin Feng Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/blog/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/blog/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/blog/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=1.0.0"></script></div></body></html>