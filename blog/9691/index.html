<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="你好，世界！"><title>深入理解神经网络中的Padding和Masking | Mr.Feng Blog</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深入理解神经网络中的Padding和Masking</h1><a id="logo" href="/blog/.">Mr.Feng Blog</a><p class="description">NLP、深度学习、机器学习、Python、Go</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">深入理解神经网络中的Padding和Masking</h1><div class="post-meta">Mar 20, 2019<span> | </span><span class="category"><a href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#padding和masking"><span class="toc-number">1.</span> <span class="toc-text">padding和masking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#padding"><span class="toc-number">1.1.</span> <span class="toc-text">padding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#masking及其计算"><span class="toc-number">1.2.</span> <span class="toc-text">masking及其计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#受-padding-影响的操作"><span class="toc-number">2.</span> <span class="toc-text">受 padding 影响的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#average"><span class="toc-number">2.1.</span> <span class="toc-text">average</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#max-amp-min"><span class="toc-number">2.2.</span> <span class="toc-text">max &amp; min</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax"><span class="toc-number">2.3.</span> <span class="toc-text">softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#log"><span class="toc-number">2.4.</span> <span class="toc-text">log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#倒数"><span class="toc-number">2.5.</span> <span class="toc-text">倒数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Layer-support-masking"><span class="toc-number">3.</span> <span class="toc-text">Layer support masking</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#情况-1"><span class="toc-number">3.1.</span> <span class="toc-text">情况 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#情况-2"><span class="toc-number">3.2.</span> <span class="toc-text">情况 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#情况-3"><span class="toc-number">3.3.</span> <span class="toc-text">情况 3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#情况-4"><span class="toc-number">3.4.</span> <span class="toc-text">情况 4</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现上的问题"><span class="toc-number">4.</span> <span class="toc-text">实现上的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#补充"><span class="toc-number">5.</span> <span class="toc-text">补充</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><p>本文讲述变长序列中的padding和masking，并展开讨论收到padding影响的操作如何处理。最后谈及Tensorflow在处理padding和masking上的实践总结。</p>
<a id="more"></a>
<p><strong>更新</strong>：后期整理了有关的代码细节，可参看<a href="https://github.com/allenwind/tensorflow-padding-masking" target="_blank" rel="noopener">Github</a>。</p>
<p>在<a href="../8912">理解神经网络中Embedding层的原理</a>中提到Embedding层的原理，而在该层后，常常面临对不定长序列数据的padding和masking处理。为此引申出本文。</p>
<h2 id="padding和masking"><a href="#padding和masking" class="headerlink" title="padding和masking"></a>padding和masking</h2><p>padding和masking示例，</p>
<p><img src="../images/padding-masking-demo-0-1.png" alt></p>
<p>padding：对不定长的文本序列进行填充，变为定长的序列，以便神经网络进行批量化学习</p>
<p>masking：指示定长的序列中哪些是原本文的数据，哪些是padding后的数据，以便神经网络区分</p>
<h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>神经网络的输入需要一个规整的张量，但是很多情况下是数据本身无法规整。例如句子，有长有短。比如下面的样本，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">  [<span class="string">"Hello"</span>, <span class="string">"world"</span>, <span class="string">"!"</span>],</span><br><span class="line">  [<span class="string">"How"</span>, <span class="string">"are"</span>, <span class="string">"you"</span>, <span class="string">"doing"</span>, <span class="string">"today"</span>],</span><br><span class="line">  [<span class="string">"The"</span>, <span class="string">"weather"</span>, <span class="string">"will"</span>, <span class="string">"be"</span>, <span class="string">"nice"</span>, <span class="string">"tomorrow"</span>],</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>然后我们通过字到ID映射，变为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">  [<span class="number">71</span>, <span class="number">1331</span>, <span class="number">4231</span>],</span><br><span class="line">  [<span class="number">73</span>, <span class="number">8</span>, <span class="number">3215</span>, <span class="number">55</span>, <span class="number">927</span>],</span><br><span class="line">  [<span class="number">83</span>, <span class="number">91</span>, <span class="number">1</span>, <span class="number">645</span>, <span class="number">1253</span>, <span class="number">927</span>],</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>这些样本长短不一，无法直接作为一个batch输入神经网络中。为此，我们可以把每个样本都填充0，使其变为定长的样本。</p>
<p><strong>方法一</strong>：</p>
<p>在Keras中，<code>tf.keras.preprocessing.sequence.pad_sequences</code>提供padding操作，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">padded_X = tf.keras.preprocessing.sequence.pad_sequences(</span><br><span class="line">    X, padding=<span class="string">"post"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>结果为，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>padded_X</span><br><span class="line">array([[  <span class="number">71</span>, <span class="number">1331</span>, <span class="number">4231</span>,    <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">0</span>],</span><br><span class="line">       [  <span class="number">73</span>,    <span class="number">8</span>, <span class="number">3215</span>,   <span class="number">55</span>,  <span class="number">927</span>,    <span class="number">0</span>],</span><br><span class="line">       [  <span class="number">83</span>,   <span class="number">91</span>,    <span class="number">1</span>,  <span class="number">645</span>, <span class="number">1253</span>,  <span class="number">927</span>]], dtype=int32)</span><br></pre></td></tr></table></figure>
<p>post（后向填充）和pre（前向填充）的padding方式是有差别的，对于RNN，推荐使用post方法，以便使用CuDNN的实现。</p>
<p><strong>方法二</strong>：</p>
<p>padding也可以自己实现，如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_padding</span><span class="params">(X, padding=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># post padding</span></span><br><span class="line">    maxlen = max([len(x) <span class="keyword">for</span> x <span class="keyword">in</span> X])</span><br><span class="line">    padded_X = np.array([</span><br><span class="line">        np.concatenate([x, [padding] * (maxlen - len(x))])</span><br><span class="line">        <span class="keyword">if</span> len(x) &lt; maxlen <span class="keyword">else</span> x <span class="keyword">for</span> x <span class="keyword">in</span> X</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> padded_X</span><br></pre></td></tr></table></figure>
<p>按照batch进行padding，padding最大长度为batch内序列的最大长度。</p>
<p><strong>方法三</strong>：</p>
<p>使用<code>tf.data.Dataset.padded_batch</code>方法，例子如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">dl = tf.data.Dataset.from_generator(</span><br><span class="line">    generator=gen,</span><br><span class="line">    output_types=tf.int32</span><br><span class="line">).padded_batch(</span><br><span class="line">    batch_size=<span class="number">1</span>,</span><br><span class="line">    padded_shapes=[<span class="number">10</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> iter(dl):</span><br><span class="line">    print(i.shape)</span><br></pre></td></tr></table></figure>
<p>在大型数据集中，推荐使用这种方法。前两张方法适合小型数据集的情况下使用。</p>
<p>现在把数据规整了，可以输入神经网络，那么如何告知网络那些数据是padding，哪些是原始数据？这样就可以避免模型在训练或推断是引入噪声。</p>
<h3 id="masking及其计算"><a href="#masking及其计算" class="headerlink" title="masking及其计算"></a>masking及其计算</h3><p>让神经网络知道那些是真实的数据，那些是padding数据。为找出mask的数据，可以用一下几种方法。这些方法都很灵活，可以根据数据特点和常见使用。</p>
<p><strong>方法一</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.not_equal(padded_X, <span class="number">0</span>)</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">6</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p><strong>方法二</strong>：</p>
<p><code>tf.sequence_mask</code>提供更方便的方法，如下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>X</span><br><span class="line">[[<span class="number">71</span>, <span class="number">1331</span>, <span class="number">4231</span>], [<span class="number">73</span>, <span class="number">8</span>, <span class="number">3215</span>, <span class="number">55</span>, <span class="number">927</span>], [<span class="number">83</span>, <span class="number">91</span>, <span class="number">1</span>, <span class="number">645</span>, <span class="number">1253</span>, <span class="number">927</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lengths = [len(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.sequence_mask(lengths)</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">6</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p><strong>方法三</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>embedding = tf.keras.layers.Embedding(input_dim=<span class="number">10000</span>, output_dim=<span class="number">8</span>, mask_</span><br><span class="line">zero=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>masked_output = embedding(padded_X)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>masked_output._keras_mask</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">6</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p><strong>方法四</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>masking = tf.keras.layers.Masking(mask_value=<span class="number">0.0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>unmasked_embedding = tf.cast(</span><br><span class="line"><span class="meta">... </span>    tf.tile(tf.expand_dims(padded_X, axis=<span class="number">-1</span>), [<span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>]), tf.float32</span><br><span class="line"><span class="meta">... </span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>masked_embedding = masking(unmasked_embedding)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>masked_embedding._keras_mask</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">6</span>), dtype=bool, numpy=</span><br><span class="line">array([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">       [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])&gt;</span><br></pre></td></tr></table></figure>
<p><strong>方法五</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = tf.greater(padded_X, <span class="number">0</span>)</span><br><span class="line">print(mask)</span><br></pre></td></tr></table></figure>
<p><strong>方法六</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mask = tf.math.logical_not(tf.math.equal(padded_X, <span class="number">0</span>))</span><br><span class="line">print(mask)</span><br></pre></td></tr></table></figure>
<p><strong>方法七</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mask_layer = tf.keras.layers.Masking(mask_value=<span class="number">0</span>)</span><br><span class="line">X = tf.expand_dims(padded_X, axis=<span class="number">-1</span>)</span><br><span class="line">mask_tensor = mask_layer.compute_mask(X)</span><br><span class="line">print(mask_tensor)</span><br></pre></td></tr></table></figure>
<p>这里提供七种方法计算Mask，使用那种可以根据数据特点和具体情况灵活选择。有时候<code>mask</code>需要广播到其他维度，这个需要另外处理。</p>
<h2 id="受-padding-影响的操作"><a href="#受-padding-影响的操作" class="headerlink" title="受 padding 影响的操作"></a>受 padding 影响的操作</h2><p>某些操作受到 padding 带来的影响，需要 masking 处理以告知某一层哪些是padded的数据，以便在计算的时候将其排除掉。</p>
<h3 id="average"><a href="#average" class="headerlink" title="average"></a>average</h3><p>平均函数如果直接求解是不准确的，应该使用如下方法，</p>
<script type="math/tex; mode=display">
\text{avg}(x) = \frac{\text{sum}(x\otimes m)}{\text{sum}(m)}</script><p>其中m表示mask。</p>
<p>因此，当实现GlobalAveragePooling时，内部涉及average操作，因此需要处理mask。</p>
<h3 id="max-amp-min"><a href="#max-amp-min" class="headerlink" title="max &amp; min"></a>max &amp; min</h3><p>如果是0，则直接乘以mask；如果是1，则在padding部分减去一个大正数，于是，</p>
<script type="math/tex; mode=display">
\max(x) = \max(x - (1-m) \times 10^{12})</script><p>根据max性质，</p>
<script type="math/tex; mode=display">
\min(x) = \min(x + (1-m) \times 10^{12})</script><p>就能获得min的mask形式。不过在神经网络中似乎很少使用这种mask。</p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><script type="math/tex; mode=display">
\operatorname{softmax}(x) = \operatorname{softmax}(x - (1-m) \times 10^{12})</script><p>注意到softmax需要计算$\exp(x)$，那么mask只需要减去一个大数即可，以Tensorflow为例，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mask = tf.expand_dims(tf.cast(mask, <span class="string">"float32"</span>), <span class="number">-1</span>)</span><br><span class="line">x = x - (<span class="number">1</span> - mask) * <span class="number">1e12</span></span><br><span class="line"><span class="comment"># 权重归一化</span></span><br><span class="line">x = tf.math.softmax(x, <span class="number">1</span>) <span class="comment"># 有mask位置对应的权重变为很小的值</span></span><br></pre></td></tr></table></figure>
<p>如果你不放心这种计算，不妨验证一下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.exp([<span class="number">-1e12</span>])</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>,), dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<p>如果框架本身没有softmax函数，可以自行实现，如numpy下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x, axis=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    x = x - x.max(axis=axis, keepdims=<span class="literal">True</span>)</span><br><span class="line">    x = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> x / x.sum(axis=axis, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里有一定的技巧，为避免数值溢出，先减去一个最大数。类似地，logsumexp也是这个原理。</p>
<h3 id="log"><a href="#log" class="headerlink" title="log"></a>log</h3><p>log的mask处理可以在mask位置加一，</p>
<script type="math/tex; mode=display">
\log(x) = \log(x + (1 - m) \times 1)</script><h3 id="倒数"><a href="#倒数" class="headerlink" title="倒数"></a>倒数</h3><p>倒数的mask处理是mask位置加上或减去一个大数，</p>
<script type="math/tex; mode=display">
\frac{1}{x} = \frac{1}{x + (1-m) \times 10^{12}}</script><h2 id="Layer-support-masking"><a href="#Layer-support-masking" class="headerlink" title="Layer support masking"></a>Layer support masking</h2><p>如果使用，Functional API 和 Sequential API，Keras中内置的层能自动接收和处理mask。Tensorflow内置的部分Layer都支持mask参数输入，以便处理padding数据。例如RNN，LSTM等等。</p>
<h3 id="情况-1"><a href="#情况-1" class="headerlink" title="情况 1"></a>情况 1</h3><p>如果自定义层需要mask信息做更多处理，例如在<a href="../9691/#受-padding-影响的操作">受 padding 影响的操作</a>。这也是在实践中遇到的最重要的情况。</p>
<p>以AttentionPooling1D为例，自定义层需要<code>call(self, inputs, mask=None)</code>有mask参数以便下层传递mask能够接收。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionPooling1D</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h_dim, kernel_initializer=<span class="string">"glorot_uniform"</span>, **kwargs)</span>:</span></span><br><span class="line">        super(AttentionPooling1D, self).__init__(**kwargs)</span><br><span class="line">        self.h_dim = h_dim</span><br><span class="line">        self.kernel_initializer = kernel_initializer</span><br><span class="line">        <span class="comment"># time steps dim change</span></span><br><span class="line">        self.supports_masking = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.k_dense = tf.keras.layers.Dense(</span><br><span class="line">            units=self.h_dim, </span><br><span class="line">            use_bias=<span class="literal">False</span>, </span><br><span class="line">            kernel_initializer=self.kernel_initializer, </span><br><span class="line">            activation=<span class="string">"tanh"</span></span><br><span class="line">        )</span><br><span class="line">        self.o_dense = tf.keras.layers.Dense(</span><br><span class="line">            units=<span class="number">1</span>, </span><br><span class="line">            use_bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 扩展维度便于广播</span></span><br><span class="line">            mask = tf.expand_dims(tf.cast(mask, <span class="string">"float32"</span>), <span class="number">-1</span>)</span><br><span class="line">        x0 = inputs</span><br><span class="line">        <span class="comment"># 计算每个 time steps 权重</span></span><br><span class="line">        x = self.k_dense(inputs)</span><br><span class="line">        x = self.o_dense(x)</span><br><span class="line">        <span class="comment"># 处理 mask</span></span><br><span class="line">        x = x - (<span class="number">1</span> - mask) * <span class="number">1e12</span></span><br><span class="line">        <span class="comment"># 权重归一化</span></span><br><span class="line">        x = tf.math.softmax(x, <span class="number">1</span>) <span class="comment"># 有mask位置对应的权重变为很小的值</span></span><br><span class="line">        <span class="comment"># 加权平均</span></span><br><span class="line">        x = tf.reduce_sum(x * x0, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (<span class="literal">None</span>, self.h_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inputs = keras.Input(shape=(<span class="literal">None</span>,), dtype=<span class="string">"int32"</span>)</span><br><span class="line">x = layers.Embedding(input_dim=<span class="number">10</span>, output_dim=<span class="number">32</span>, mask_zero=<span class="literal">True</span>)(inputs)</span><br><span class="line">x = layers.Dense(<span class="number">1</span>)(x)</span><br><span class="line">outputs = TemporalSoftmax()(x)</span><br><span class="line"></span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line">y = model(np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">32</span>, <span class="number">100</span>)), np.random.random((<span class="number">32</span>, <span class="number">100</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="情况-2"><a href="#情况-2" class="headerlink" title="情况 2"></a>情况 2</h3><p>如果自定义层并没有破坏输入形状<code>(samples, timesteps, features)</code>中的时间维度，但内部组件需要接收mask，可以使用直接传递mask的方法。这种用法和情况一一样，只不过这里使用的是内置的层。AttentionPooling1D也可以作为这里的基本组件，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMClassifier</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h_dims=<span class="number">128</span>, input_dim=<span class="number">100000</span>, output_dim=<span class="number">128</span>, **kwargs)</span>:</span></span><br><span class="line">        super(LSTMClassifier, self).__init__(**kwargs)</span><br><span class="line">        self.h_dims = h_dims</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(</span><br><span class="line">            input_dim=self.input_dim, </span><br><span class="line">            output_dim=self.output_dim, </span><br><span class="line">            mask_zero=<span class="literal">True</span> <span class="comment"># 指定mask值</span></span><br><span class="line">        )</span><br><span class="line">        self.lstm = tf.keras.layers.LSTM(self.h_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        x = self.embedding(inputs)</span><br><span class="line">        <span class="comment"># 计算mask</span></span><br><span class="line">        mask = self.embedding.compute_mask(inputs)</span><br><span class="line">        <span class="keyword">return</span> self.lstm(x, mask=mask) <span class="comment"># lstm会忽略padding值</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (<span class="literal">None</span>, self.h_dims)</span><br><span class="line"></span><br><span class="line">layer = MyLayer()</span><br><span class="line">x = np.random.random((<span class="number">32</span>, <span class="number">10</span>)) * <span class="number">100</span></span><br><span class="line">x = x.astype(<span class="string">"int32"</span>)</span><br><span class="line">layer(x)</span><br></pre></td></tr></table></figure>
<h3 id="情况-3"><a href="#情况-3" class="headerlink" title="情况 3"></a>情况 3</h3><p>如果自定义层的功能破坏了原有输入形状<code>(samples, timesteps, features)</code>中的时间维度，如Flatten操作等，需要实现compute_mask方法，因为新的形状不知道对应的mask是什么，需要重新计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalSplit</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""Split the input tensor into 2 tensors along the time dimension."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># Expect the input to be 3D and mask to be 2D, split the input tensor into 2</span></span><br><span class="line">        <span class="comment"># subtensors along the time axis (axis 1).</span></span><br><span class="line">        <span class="keyword">return</span> tf.split(inputs, <span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_mask</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># Also split the mask into 2 if it presents.</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> tf.split(mask, <span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">first_half, second_half = TemporalSplit()(masked_embedding)</span><br><span class="line">print(first_half._keras_mask)</span><br><span class="line">print(second_half._keras_mask)</span><br></pre></td></tr></table></figure>
<p>例如双向的LSTM层，对mask发生改变</p>
<h3 id="情况-4"><a href="#情况-4" class="headerlink" title="情况 4"></a>情况 4</h3><p>如果是自定义层，则需要分情况讨论。</p>
<p>Embedding mask_zero=True</p>
<p>设置Masking Layer</p>
<p>如果自定义层并没有破坏输入形状<code>(samples, timesteps, features)</code>中的时间维度，为了让mask向其他层传播，需要在自定义层设置<code>self.supports_masking = True</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyActivation</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(MyActivation, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># Signal that the layer is safe for mask propagation</span></span><br><span class="line">        self.supports_masking = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.relu(inputs)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">inputs = keras.Input(shape=(<span class="literal">None</span>,), dtype=<span class="string">"int32"</span>)</span><br><span class="line">x = layers.Embedding(input_dim=<span class="number">5000</span>, output_dim=<span class="number">16</span>, mask_zero=<span class="literal">True</span>)(inputs)</span><br><span class="line">x = MyActivation()(x)  <span class="comment"># Will pass the mask along</span></span><br><span class="line">print(<span class="string">"Mask found:"</span>, x._keras_mask)</span><br><span class="line">outputs = layers.LSTM(<span class="number">32</span>)(x)  <span class="comment"># Will receive the mask</span></span><br><span class="line"></span><br><span class="line">model = keras.Model(inputs, outputs)</span><br></pre></td></tr></table></figure>
<h2 id="实现上的问题"><a href="#实现上的问题" class="headerlink" title="实现上的问题"></a>实现上的问题</h2><p>一种常见的方法是数据和mask都放到inputs上，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XMaskedLayer</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x, mask = inputs</span><br></pre></td></tr></table></figure>
<p>应该把inputs和mask区分开来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XMaskedLayer</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask)</span>:</span></span><br><span class="line">        x1, x2 = inputs</span><br><span class="line">        a_mask, b_mask = mask</span><br></pre></td></tr></table></figure>
<p>这里一BiLSTM作为示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskBiLSTM</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""支持mask的BiLSTM"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hdims, **kwargs)</span>:</span></span><br><span class="line">        super(MaskBiLSTM, self).__init__(**kwargs)</span><br><span class="line">        self.hdims = hdims</span><br><span class="line">        self.forward_lstm = LSTM(hdims, return_sequences=<span class="literal">True</span>)</span><br><span class="line">        self.backend_lstm = LSTM(hdims, return_sequences=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse_sequence</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        seq_len = tf.reduce_sum(mask, axis=<span class="number">1</span>)[:, <span class="number">0</span>]</span><br><span class="line">        seq_len = tf.cast(seq_len, tf.int32)</span><br><span class="line">        x = tf.reverse_sequence(x, seq_len, seq_axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            mask = <span class="number">1.0</span></span><br><span class="line">        x = inputs</span><br><span class="line">        x_forward = self.forward_lstm(x)</span><br><span class="line">        x_backward = self.reverse_sequence(x, mask)</span><br><span class="line">        x_backward = self.backend_lstm(x_backward)</span><br><span class="line">        x_backward = self.reverse_sequence(x_backward, mask)</span><br><span class="line">        x = tf.concat([x_forward, x_backward], axis=<span class="number">-1</span>)</span><br><span class="line">        x = x * mask</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> input_shape[<span class="number">0</span>][:<span class="number">-1</span>] + (self.hdims * <span class="number">2</span>,)</span><br></pre></td></tr></table></figure>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>以上，很多的代码细节，后期已经好了，放到Github，有需要请参看<a href="https://github.com/allenwind/tensorflow-padding-masking" target="_blank" rel="noopener">tensorflow-padding-masking</a>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>因此Keras中masking和padding的使用规律如下，</p>
<ul>
<li>如果一个layer受padding的影响，那么在call方法中传入mask，或者在call方法内计算出mask，并根据具体的操作mask掉padding的影响，如上述提到的softmax的处理方式。</li>
<li>如果一个layer对输入数据的timesteps维度产生影响，为让上层知道这个影响，mask也需要相应的处理。</li>
<li>如果一个layer内部的组件需要mask来处理padding，如果该组件支持处理mask，那么获得mask后，直接传入即可</li>
<li>如果一个layer并不受padding的影响，但是mask信息需要往上传递，那么让<code>self.supports_masking=True</code></li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://tensorflow.google.cn/guide/keras/masking_and_padding?hl=en" target="_blank" rel="noopener">https://tensorflow.google.cn/guide/keras/masking_and_padding?hl=en</a></p>
<p>[2] <a href="https://github.com/allenwind/tensorflow-padding-masking" target="_blank" rel="noopener">tensorflow-padding-masking</a></p>
</div><div class="tags"><a href="/blog/tags/Tensorflow/">Tensorflow</a><a href="/blog/tags/深度学习/">深度学习</a><a href="/blog/tags/神经网络/">神经网络</a></div><div class="post-nav"><a class="pre" href="/blog/9754/">使用Conda或Docker创建多版本Python环境</a><a class="next" href="/blog/9606/">信息聚合漫谈：加权平均思路</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://allenwind.github.io/blog"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/C-C/">C/C++</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据结构和算法/">数据结构和算法</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/记录/">记录</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/C语言/" style="font-size: 15px;">C语言</a> <a href="/blog/tags/算法/" style="font-size: 15px;">算法</a> <a href="/blog/tags/C/" style="font-size: 15px;">C</a> <a href="/blog/tags/Python/" style="font-size: 15px;">Python</a> <a href="/blog/tags/设计模式/" style="font-size: 15px;">设计模式</a> <a href="/blog/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/blog/tags/数据库/" style="font-size: 15px;">数据库</a> <a href="/blog/tags/区块链/" style="font-size: 15px;">区块链</a> <a href="/blog/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/blog/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/blog/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/blog/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/blog/tags/sudo/" style="font-size: 15px;">sudo</a> <a href="/blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/blog/tags/git/" style="font-size: 15px;">git</a> <a href="/blog/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/blog/tags/Git/" style="font-size: 15px;">Git</a> <a href="/blog/tags/gcc/" style="font-size: 15px;">gcc</a> <a href="/blog/tags/Systemd/" style="font-size: 15px;">Systemd</a> <a href="/blog/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/blog/tags/命令行/" style="font-size: 15px;">命令行</a> <a href="/blog/tags/网络/" style="font-size: 15px;">网络</a> <a href="/blog/tags/记录/" style="font-size: 15px;">记录</a> <a href="/blog/tags/操作系统/" style="font-size: 15px;">操作系统</a> <a href="/blog/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/blog/tags/数据结构/" style="font-size: 15px;">数据结构</a> <a href="/blog/tags/Go/" style="font-size: 15px;">Go</a> <a href="/blog/tags/go/" style="font-size: 15px;">go</a> <a href="/blog/tags/Java/" style="font-size: 15px;">Java</a> <a href="/blog/tags/LRU/" style="font-size: 15px;">LRU</a> <a href="/blog/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/blog/tags/剑指Offer/" style="font-size: 15px;">剑指Offer</a> <a href="/blog/tags/函数式/" style="font-size: 15px;">函数式</a> <a href="/blog/tags/搜索引擎/" style="font-size: 15px;">搜索引擎</a> <a href="/blog/tags/Stack/" style="font-size: 15px;">Stack</a> <a href="/blog/tags/并发编程/" style="font-size: 15px;">并发编程</a> <a href="/blog/tags/网络编程/" style="font-size: 15px;">网络编程</a> <a href="/blog/tags/并发/" style="font-size: 15px;">并发</a> <a href="/blog/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/blog/tags/装饰器/" style="font-size: 15px;">装饰器</a> <a href="/blog/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/blog/tags/数学/" style="font-size: 15px;">数学</a> <a href="/blog/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/blog/tags/投资/" style="font-size: 15px;">投资</a> <a href="/blog/tags/概率/" style="font-size: 15px;">概率</a> <a href="/blog/tags/统计/" style="font-size: 15px;">统计</a> <a href="/blog/tags/信息论/" style="font-size: 15px;">信息论</a> <a href="/blog/tags/度量/" style="font-size: 15px;">度量</a> <a href="/blog/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/blog/tags/贝叶斯/" style="font-size: 15px;">贝叶斯</a> <a href="/blog/tags/最小二乘法/" style="font-size: 15px;">最小二乘法</a> <a href="/blog/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/blog/tags/变分推断/" style="font-size: 15px;">变分推断</a> <a href="/blog/tags/随机/" style="font-size: 15px;">随机</a> <a href="/blog/tags/不等式/" style="font-size: 15px;">不等式</a> <a href="/blog/tags/采样/" style="font-size: 15px;">采样</a> <a href="/blog/tags/分布/" style="font-size: 15px;">分布</a> <a href="/blog/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/blog/tags/光滑/" style="font-size: 15px;">光滑</a> <a href="/blog/tags/逼近/" style="font-size: 15px;">逼近</a> <a href="/blog/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/blog/tags/交叉验证/" style="font-size: 15px;">交叉验证</a> <a href="/blog/tags/词向量/" style="font-size: 15px;">词向量</a> <a href="/blog/tags/信息/" style="font-size: 15px;">信息</a> <a href="/blog/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/blog/tags/分类/" style="font-size: 15px;">分类</a> <a href="/blog/tags/证明/" style="font-size: 15px;">证明</a> <a href="/blog/tags/损失函数/" style="font-size: 15px;">损失函数</a> <a href="/blog/tags/回归/" style="font-size: 15px;">回归</a> <a href="/blog/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/blog/tags/指标/" style="font-size: 15px;">指标</a> <a href="/blog/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/blog/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/blog/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/blog/tags/Transformer/" style="font-size: 15px;">Transformer</a> <a href="/blog/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/blog/tags/对抗训练/" style="font-size: 15px;">对抗训练</a> <a href="/blog/tags/文本分类/" style="font-size: 15px;">文本分类</a> <a href="/blog/tags/分词/" style="font-size: 15px;">分词</a> <a href="/blog/tags/并行/" style="font-size: 15px;">并行</a> <a href="/blog/tags/NER/" style="font-size: 15px;">NER</a> <a href="/blog/tags/概率图/" style="font-size: 15px;">概率图</a> <a href="/blog/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/blog/tags/图/" style="font-size: 15px;">图</a> <a href="/blog/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/blog/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/blog/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/blog/tags/梯度/" style="font-size: 15px;">梯度</a> <a href="/blog/tags/注意力/" style="font-size: 15px;">注意力</a> <a href="/blog/tags/序列编码/" style="font-size: 15px;">序列编码</a> <a href="/blog/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/blog/tags/磁盘故障/" style="font-size: 15px;">磁盘故障</a> <a href="/blog/tags/优化/" style="font-size: 15px;">优化</a> <a href="/blog/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/blog/tags/卷积/" style="font-size: 15px;">卷积</a> <a href="/blog/tags/调参/" style="font-size: 15px;">调参</a> <a href="/blog/tags/Flask/" style="font-size: 15px;">Flask</a> <a href="/blog/tags/web/" style="font-size: 15px;">web</a> <a href="/blog/tags/socket/" style="font-size: 15px;">socket</a> <a href="/blog/tags/安全/" style="font-size: 15px;">安全</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/16051/">机器学习之分类问题的评估指标总结</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16003/">天马行空：设计自己的激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15887/">函数光滑近似（4）：Heaviside step函数及其应用</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15205/">引入参数控制softmax的smooth程度</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15110/">分析与拓展：多分类模型的输出为什么使用softmax？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14990/">一种基于光滑逼近的正态分布采样法</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14721/">GELU由来：从狄拉克函数到GELU激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14465/">Lp范数的上下界分析</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14441/">logsumexp函数分析</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14133/">分析Mish激活函数的设计思路</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/allenwind" title="My GitHub" target="_blank">My GitHub</a><ul></ul><a href="http://www.arxiv-sanity.com/" title="arxiv-sanity" target="_blank">arxiv-sanity</a><ul></ul><a href="https://arxiv.org/" title="arxiv.org" target="_blank">arxiv.org</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/blog/." rel="nofollow">Mr.Feng Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>