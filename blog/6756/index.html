<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="你好，世界！"><title>频率学派与贝叶斯学派之争 | Mr.Feng Blog</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">频率学派与贝叶斯学派之争</h1><a id="logo" href="/blog/.">Mr.Feng Blog</a><p class="description">NLP、深度学习、机器学习、Python、Go</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">频率学派与贝叶斯学派之争</h1><div class="post-meta">Sep 7, 2018<span> | </span><span class="category"><a href="/blog/categories/数学/">数学</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#频率学派的极大似然估计（MLE）"><span class="toc-number">1.</span> <span class="toc-text">频率学派的极大似然估计（MLE）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#贝叶斯学派的最大后验估计（MAPE）"><span class="toc-number">2.</span> <span class="toc-text">贝叶斯学派的最大后验估计（MAPE）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#频率学派与贝叶斯学派之争"><span class="toc-number">3.</span> <span class="toc-text">频率学派与贝叶斯学派之争</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#频率学派"><span class="toc-number">3.1.</span> <span class="toc-text">频率学派</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝叶斯学派"><span class="toc-number">3.2.</span> <span class="toc-text">贝叶斯学派</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一点联想"><span class="toc-number">4.</span> <span class="toc-text">一点联想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">6.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><p>频率学派与贝叶斯学派在参数估计中分别对应最大似然估计（MLE）与最大后验估计（MAPE），它们都是机器学习和数理统计中常见的参数估计方法。今天我们先分析一下频率学派与贝叶斯学派的出发点和基本思想差异。</p>
<a id="more"></a>
<h2 id="频率学派的极大似然估计（MLE）"><a href="#频率学派的极大似然估计（MLE）" class="headerlink" title="频率学派的极大似然估计（MLE）"></a>频率学派的极大似然估计（MLE）</h2><p>首先我们从频率学派的最大似然预计（MLE）出发，然后从若干角度理解该参数估计方法的机理。极大似然估计（MLE）是一种参数估计方法，在假定样本数据独立同分布情况下，求出似然函数极大值时的参数，该参数即为样本数据分布函数的参数的一个估计值。</p>
<p>假设有概率分布$f(x, \theta)$，从该分布中采样$n$个样本$x_{1}, \dots, x_{n}$。易得似然函数，</p>
<script type="math/tex; mode=display">
L_{n}(x_1,\dots, x_{n}, \theta) = \prod_{i=1}^{n} f(x_{i}, \theta)</script><p>MLE寻找让似然函数最大的参数，即，</p>
<script type="math/tex; mode=display">
\hat {\theta }=\underset {\theta \in \Theta }{\operatorname {arg\;max} } \; L_{n}(x_1,\dots, x_{n}, \theta)</script><p>也就是在假定$\theta$固定但未知的情况下，求最值。这个思路很直观吧，但是贝叶斯学派的最大后验估计方法则有点让人吃惊。</p>
<h2 id="贝叶斯学派的最大后验估计（MAPE）"><a href="#贝叶斯学派的最大后验估计（MAPE）" class="headerlink" title="贝叶斯学派的最大后验估计（MAPE）"></a>贝叶斯学派的最大后验估计（MAPE）</h2><p>以上的讨论让我们意识到频率学派的最大似然估计（MLE）对参数的基本假设是，参数是一个<strong>未知但固定的常数</strong>。贝叶斯学派的最大后验估计（MAPE）则认为，参数并不是一个固定的常数，而是一个未知的随机变量，服从一个概率分布，称为先验分布。有概率分布$f(x, \theta)$，从该分布中采样$n$个样本$x_{1}, \dots, x_{n}$。这里假设参数$\theta$的先验分布为$h(\theta)$，那么根据贝叶斯公式，参数$\theta$的后验分布为，</p>
<script type="math/tex; mode=display">
h(\theta | x_1, \dots, x_n) = \frac{h(\theta) f(x_1, \theta) \cdots f(x_n, \theta)}{p(x_1, x_2, \dots, x_n)}</script><p>这里，</p>
<script type="math/tex; mode=display">
p(x_1, x_2, \dots, x_n) = \int_{\Theta}h(\theta) f(x_1, \theta) \cdots f(x_n, \theta) \; d\theta</script><p>考虑到参数$\theta$的后验分布是一个函数，那么实际使用中，需要$\theta$的具体值，那么后验分布的均值或众数或中位数（posterior mean、posteriormode、posterior median）也可以，这个使用的选择性比较灵活。例如，后验分布的众数可以表示为，</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\theta} 
&= \arg \max_{\theta} \; \frac{h(\theta) f(x_1, \theta) \cdots f(x_n, \theta)}{\int_{\Theta}h(\theta) f(x_1, \theta) \cdots f(x_n, \theta) \; d\theta} \newline
&= \arg \max_{\theta} \; h(\theta) f(x_1, \theta) \cdots f(x_n, \theta)
\end{align}</script><p>这里后验分布的分母与$\theta$无关，所有可以去掉。这里也可以看到，当$h(\theta) = \text{const}$时，极大似然估计等价于最大后验估计。换句话说，极大似然估计等价于先验分布为均匀分布的最大后验估计。</p>
<h2 id="频率学派与贝叶斯学派之争"><a href="#频率学派与贝叶斯学派之争" class="headerlink" title="频率学派与贝叶斯学派之争"></a>频率学派与贝叶斯学派之争</h2><p>机器学习中各模型的学习过程实质上是通过已有数据计算给定模型的未知参数，对于统计模型，学习过程则是一个参数估计问题。在统计学上，参数估计分为两大类方法：</p>
<ul>
<li>频率方法</li>
<li>贝叶斯方法</li>
</ul>
<p>这两种方法对未知参数的看待有着本质的区别。</p>
<h3 id="频率学派"><a href="#频率学派" class="headerlink" title="频率学派"></a>频率学派</h3><p>在参数估计中，频率学派统计学的观点是，分布函数（或密度函数）中的真实参数未知但为定值，通过对样本数据的计算就可以获得这个真实参数的估计值。具体的做法是使用最大似然估计，大致思路如下：假定数据集（样本）是独立同分布，计算数据集的联合概率分布，由于数据是独立同分布，那么计算结果为各样本点分布函数的乘积，称为似然函数。有时候为了方便计算，会对似然函数取对数。使似然函数取最大值的参数即为我们要估计的参数，称为最大似然估计量。</p>
<p>直观上很容易理解这种参数估计方法，即似然函数取得极大值的估计参数是获得这样的样本观察结果的原因，如果不是这样的参数就不会有这样的观察结果。换句话说，已经取得这样的观察样本，是因为观察到这样的样本的概率很大，即似然函数很大。</p>
<p>然而，观察样本的可能组合非常大，甚至无穷，最大似然估计量依赖于观察的样本，怎样能够保证最大似然估计取得的估计量接近真实的参数值？我们有什么理由认为它是合理的？</p>
<p>可是观察结果有很多种（每次观察都可以看做是随机过程的一个样本函数），怎样保证参数是唯一的。于是就有了参数的区间估计。根据频率学派的观点，真实分布的参数是固定。于是，估计参数的不确定性是观察导致的，既然如此，我们有什么理解相信基于极大似然估计得到的参数是有效的。</p>
<p>换个角度看，我们要找一个尽可能和真实数据生成分布相近的分布。在信息论上，相对熵的概念可以度量两个分布之间的差异。根据定义，在真实分布确定但未知的情况下，让相对熵最少就是让带寻找的分布函数最大（似然函数）。</p>
<h3 id="贝叶斯学派"><a href="#贝叶斯学派" class="headerlink" title="贝叶斯学派"></a>贝叶斯学派</h3><p>类似地，在贝叶斯学派中有类似的方法，最大后验估计，贝叶斯学派在抽样前对参数$\theta$就有了解，称为先验信息，该参数可以用一个概率分布$p(\theta)$来描述。$p(\theta)$的具体形式事先必须给出。</p>
<p>贝叶斯学派与频率学派在参数估计中的差异可以用下表概况，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>学派</th>
<th>参数特点</th>
<th>代表方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>频率主义学派</td>
<td>未知但为定值</td>
<td>距估计、最大似然估计</td>
</tr>
<tr>
<td>贝叶斯学派</td>
<td>随机变量但服从先验分布</td>
<td>贝叶斯估计</td>
</tr>
</tbody>
</table>
</div>
<p>然而，贝叶斯学派却拒绝回答中间结果的现实意义，如果使用频率学说解释，那么就跳进频率学派的基本观点上。这大概就是贝叶斯学派被频率学派抓到的把柄。</p>
<h2 id="一点联想"><a href="#一点联想" class="headerlink" title="一点联想"></a>一点联想</h2><p>参数估计的频率学派与贝叶斯学派的争议，容易让人联想到量子力学中两个学派的争议，</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>学派</th>
<th>波函数意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>正统学派</td>
<td>测量本身创造这种统计分布</td>
</tr>
<tr>
<td>现实主义学派</td>
<td>物理系统在测量之前就存在统计分布</td>
</tr>
</tbody>
</table>
</div>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了参数估计中常见的最大似然估计（MLE）与最大后验估计（MAPE），并对比频率学派与贝叶斯学派的出发点和基本思想差异。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] 《概率论与数理统计》陈希孺</p>
<p>[2] 《数理统计学教程》陈希孺</p>
<p>[3] <a href="https://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/lectures/lecture03.pdf" target="_blank" rel="noopener">https://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-MT2016/lectures/lecture03.pdf</a></p>
</div><div class="tags"><a href="/blog/tags/概率/">概率</a><a href="/blog/tags/统计/">统计</a><a href="/blog/tags/机器学习/">机器学习</a><a href="/blog/tags/贝叶斯/">贝叶斯</a><a href="/blog/tags/最小二乘法/">最小二乘法</a></div><div class="post-nav"><a class="pre" href="/blog/6781/">极大似然估计与最小二乘法的关系</a><a class="next" href="/blog/6708/">最大熵原理、最大熵约束与概率分布</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://allenwind.github.io/blog"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/C-C/">C/C++</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据结构和算法/">数据结构和算法</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/记录/">记录</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/算法/" style="font-size: 15px;">算法</a> <a href="/blog/tags/C/" style="font-size: 15px;">C</a> <a href="/blog/tags/Python/" style="font-size: 15px;">Python</a> <a href="/blog/tags/设计模式/" style="font-size: 15px;">设计模式</a> <a href="/blog/tags/C语言/" style="font-size: 15px;">C语言</a> <a href="/blog/tags/数据库/" style="font-size: 15px;">数据库</a> <a href="/blog/tags/区块链/" style="font-size: 15px;">区块链</a> <a href="/blog/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/blog/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/blog/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/blog/tags/sudo/" style="font-size: 15px;">sudo</a> <a href="/blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/blog/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/blog/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/blog/tags/Git/" style="font-size: 15px;">Git</a> <a href="/blog/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/blog/tags/git/" style="font-size: 15px;">git</a> <a href="/blog/tags/命令行/" style="font-size: 15px;">命令行</a> <a href="/blog/tags/gcc/" style="font-size: 15px;">gcc</a> <a href="/blog/tags/Systemd/" style="font-size: 15px;">Systemd</a> <a href="/blog/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/blog/tags/网络/" style="font-size: 15px;">网络</a> <a href="/blog/tags/操作系统/" style="font-size: 15px;">操作系统</a> <a href="/blog/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/blog/tags/记录/" style="font-size: 15px;">记录</a> <a href="/blog/tags/数据结构/" style="font-size: 15px;">数据结构</a> <a href="/blog/tags/Go/" style="font-size: 15px;">Go</a> <a href="/blog/tags/go/" style="font-size: 15px;">go</a> <a href="/blog/tags/Java/" style="font-size: 15px;">Java</a> <a href="/blog/tags/LRU/" style="font-size: 15px;">LRU</a> <a href="/blog/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/blog/tags/搜索引擎/" style="font-size: 15px;">搜索引擎</a> <a href="/blog/tags/剑指Offer/" style="font-size: 15px;">剑指Offer</a> <a href="/blog/tags/函数式/" style="font-size: 15px;">函数式</a> <a href="/blog/tags/Stack/" style="font-size: 15px;">Stack</a> <a href="/blog/tags/并发编程/" style="font-size: 15px;">并发编程</a> <a href="/blog/tags/网络编程/" style="font-size: 15px;">网络编程</a> <a href="/blog/tags/并发/" style="font-size: 15px;">并发</a> <a href="/blog/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/blog/tags/装饰器/" style="font-size: 15px;">装饰器</a> <a href="/blog/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/blog/tags/数学/" style="font-size: 15px;">数学</a> <a href="/blog/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/blog/tags/投资/" style="font-size: 15px;">投资</a> <a href="/blog/tags/度量/" style="font-size: 15px;">度量</a> <a href="/blog/tags/概率/" style="font-size: 15px;">概率</a> <a href="/blog/tags/统计/" style="font-size: 15px;">统计</a> <a href="/blog/tags/信息论/" style="font-size: 15px;">信息论</a> <a href="/blog/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/blog/tags/贝叶斯/" style="font-size: 15px;">贝叶斯</a> <a href="/blog/tags/最小二乘法/" style="font-size: 15px;">最小二乘法</a> <a href="/blog/tags/不等式/" style="font-size: 15px;">不等式</a> <a href="/blog/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/blog/tags/变分推断/" style="font-size: 15px;">变分推断</a> <a href="/blog/tags/随机/" style="font-size: 15px;">随机</a> <a href="/blog/tags/采样/" style="font-size: 15px;">采样</a> <a href="/blog/tags/逼近/" style="font-size: 15px;">逼近</a> <a href="/blog/tags/光滑/" style="font-size: 15px;">光滑</a> <a href="/blog/tags/分布/" style="font-size: 15px;">分布</a> <a href="/blog/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/blog/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/blog/tags/词向量/" style="font-size: 15px;">词向量</a> <a href="/blog/tags/信息/" style="font-size: 15px;">信息</a> <a href="/blog/tags/交叉验证/" style="font-size: 15px;">交叉验证</a> <a href="/blog/tags/分类/" style="font-size: 15px;">分类</a> <a href="/blog/tags/指标/" style="font-size: 15px;">指标</a> <a href="/blog/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/blog/tags/证明/" style="font-size: 15px;">证明</a> <a href="/blog/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/blog/tags/损失函数/" style="font-size: 15px;">损失函数</a> <a href="/blog/tags/回归/" style="font-size: 15px;">回归</a> <a href="/blog/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/blog/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/blog/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/blog/tags/Transformer/" style="font-size: 15px;">Transformer</a> <a href="/blog/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/blog/tags/对抗训练/" style="font-size: 15px;">对抗训练</a> <a href="/blog/tags/文本分类/" style="font-size: 15px;">文本分类</a> <a href="/blog/tags/分词/" style="font-size: 15px;">分词</a> <a href="/blog/tags/并行/" style="font-size: 15px;">并行</a> <a href="/blog/tags/NER/" style="font-size: 15px;">NER</a> <a href="/blog/tags/概率图/" style="font-size: 15px;">概率图</a> <a href="/blog/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/blog/tags/图/" style="font-size: 15px;">图</a> <a href="/blog/tags/梯度/" style="font-size: 15px;">梯度</a> <a href="/blog/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/blog/tags/注意力/" style="font-size: 15px;">注意力</a> <a href="/blog/tags/序列编码/" style="font-size: 15px;">序列编码</a> <a href="/blog/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/blog/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/blog/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/blog/tags/注意力机制/" style="font-size: 15px;">注意力机制</a> <a href="/blog/tags/磁盘故障/" style="font-size: 15px;">磁盘故障</a> <a href="/blog/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/blog/tags/卷积/" style="font-size: 15px;">卷积</a> <a href="/blog/tags/优化/" style="font-size: 15px;">优化</a> <a href="/blog/tags/调参/" style="font-size: 15px;">调参</a> <a href="/blog/tags/Flask/" style="font-size: 15px;">Flask</a> <a href="/blog/tags/web/" style="font-size: 15px;">web</a> <a href="/blog/tags/socket/" style="font-size: 15px;">socket</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/16228/">分析与拓展：Transformer中的MultiHeadAttention为什么使用scaled？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16113/">Tensorflow的多卡训练：原理和实践</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16051/">机器学习之分类问题的评估指标总结</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16003/">天马行空：设计自己的激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15887/">函数光滑近似（4）：Heaviside step函数及其应用</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15205/">引入参数控制softmax的smooth程度</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15110/">分析与拓展：多分类模型的输出为什么使用softmax？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14990/">一种基于光滑逼近的正态分布采样法</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14721/">GELU由来：从狄拉克函数到GELU激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14465/">Lp范数的上下界分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/allenwind" title="My GitHub" target="_blank">My GitHub</a><ul></ul><a href="http://www.arxiv-sanity.com/" title="arxiv-sanity" target="_blank">arxiv-sanity</a><ul></ul><a href="https://arxiv.org/" title="arxiv.org" target="_blank">arxiv.org</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/blog/." rel="nofollow">Mr.Feng Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>