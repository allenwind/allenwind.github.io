<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="你好，世界！"><title>文本分类中的一些Tricks分享 | Mr.Feng Blog</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">文本分类中的一些Tricks分享</h1><a id="logo" href="/blog/.">Mr.Feng Blog</a><p class="description">NLP、深度学习、机器学习、Python、Go</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">文本分类中的一些Tricks分享</h1><div class="post-meta">Dec 27, 2018<span> | </span><span class="category"><a href="/blog/categories/NLP/">NLP</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#基本架构"><span class="toc-number">1.</span> <span class="toc-text">基本架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据和标签角度"><span class="toc-number">2.</span> <span class="toc-text">数据和标签角度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类特征、句向量"><span class="toc-number">3.</span> <span class="toc-text">分类特征、句向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征编码器Encoder"><span class="toc-number">4.</span> <span class="toc-text">特征编码器Encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding与词汇信息融入（更新）"><span class="toc-number">5.</span> <span class="toc-text">Embedding与词汇信息融入（更新）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout的运用"><span class="toc-number">6.</span> <span class="toc-text">Dropout的运用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机替换tokens"><span class="toc-number">7.</span> <span class="toc-text">随机替换tokens</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LayerNormalization"><span class="toc-number">8.</span> <span class="toc-text">LayerNormalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#作为匹配问题"><span class="toc-number">9.</span> <span class="toc-text">作为匹配问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#位置编码"><span class="toc-number">10.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分词-or-不分词？"><span class="toc-number">11.</span> <span class="toc-text">分词 or 不分词？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">12.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">13.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><p>深度学习已经成为 AI 的标配技术，在 NLP 领域更是大放异彩。而NLP中分类任务是应用上最基础的任务，具体包括新闻标题分类、情感倾向性分析、情绪识别、意图识别、关系分类、事件类型判断、语义相似识别等等。本文总结一下文本分类中的一些Tricks。</p>
<a id="more"></a>
<p><strong>注</strong>：更新了一些内容~</p>
<h2 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h2><p>文本分类任务基本上沿着如下架构进行，</p>
<p><img src="../images/text-classifier-tricks-layers.png" alt></p>
<p>所有的Tricks基本上都可以归为以上五层之一。一下Tricks的介绍会按照以上五个维度来归类。</p>
<h2 id="数据和标签角度"><a href="#数据和标签角度" class="headerlink" title="数据和标签角度"></a>数据和标签角度</h2><p>从输出标签看，包括：二分类、多分类、多标签分类、多示例分类、层次化分类，</p>
<ul>
<li>Binary-class text classifcation</li>
<li>Multi-class text classification</li>
<li>Multi-label text classification</li>
<li>Hiearchical (multi-label) text classification (HMC)</li>
</ul>
<p>从X数据看：</p>
<ul>
<li>关于X：<ol>
<li>短文</li>
<li>长文</li>
<li>中英混合（多语言）</li>
<li>单语言</li>
</ol>
</li>
</ul>
<p>从y标签看：</p>
<ul>
<li>关于y：<ol>
<li>二分类</li>
<li>多分类</li>
<li>长文多分类</li>
<li>多标签分类</li>
<li>层次化标签分类</li>
<li>正负样本不平衡分类</li>
<li>长尾样本分类</li>
</ol>
</li>
</ul>
<p>类别编码常常使用one-hot表示，例如</p>
<script type="math/tex; mode=display">
c_{k} = [0, \dots, 1, \dots, 0]</script><p>是对类别$c_k$​的编码，这在NLP或其他领域中的分类任务再普通不过。标签（类别）可以理解成是引导数据压缩的方向的对象，标签平滑是文本分类中常用的tricks，使得原来的onehot变为，</p>
<script type="math/tex; mode=display">
y = [\frac{\epsilon}{K-1}, \dots,1-\epsilon,\dots,\frac{\epsilon}{K-1}]</script><p>标签平滑是在标签上添加随机噪声，</p>
<script type="math/tex; mode=display">
\mathop{\arg\min}_{\theta} L_{\varepsilon}(\theta)=
\mathbb{E}_{(x,y)\sim \mathcal{D}, \varepsilon\sim q(\varepsilon)}
[L(f(x), y + \varepsilon)]</script><p>即在原来onehot基础上添加有一定规则的噪声，如下</p>
<script type="math/tex; mode=display">
[\frac{\epsilon}{K-1}, \dots, -\epsilon, \dots, \frac{\epsilon}{K-1}]</script><p>事实上，从另外一个角度来说，标签平滑可以看做是标签带有规则的初始化方法。标签平滑看起来没有直接的实现方法，而事实上，使用Tensorflow中的one-hot就可以实现，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">5</span>, <span class="number">0.8</span>, <span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<p>对于长尾标签，某些类别的样本非常少，简单的做法是把它们归为一个新类别：其他。</p>
<h2 id="分类特征、句向量"><a href="#分类特征、句向量" class="headerlink" title="分类特征、句向量"></a>分类特征、句向量</h2><p>在输出分类类别前，即softmax计算前，都需要把向量序列转化为单个向量，这个向量称为类别向量、类别特征或者句向量。说白了，分类器，一般是 softmax，但是具体还得看输出，是 multi-label 还是 multi-class 场景。</p>
<p>常见的从词向量序列获取句向量（融合词向量序列为句向量）：</p>
<ol>
<li>GlobalMaxPooling</li>
<li>GlobalAveragePooling</li>
<li>AttentionPooling</li>
<li>MultiHeadAttentionPooling</li>
<li>MinVariacnePooling</li>
<li>BERT[CLS]</li>
</ol>
<p>每个时间步的编码特征组成向量序列，</p>
<script type="math/tex; mode=display">
[\boldsymbol{h}_{1}, \dots, \boldsymbol{h}_{n}]</script><p>由于RNN的递归编码特点，可以认为最后一个时间步整合了序列的所有信息，因此可以直接用$\boldsymbol{h}_{n}$作为特征。</p>
<p>所有时间步共同处理，直接平均</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{f} = \frac{1}{n} \sum_{i=1}^{n} \boldsymbol{h}_{i}</script><p>GlobalMax操作，</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{f} = \operatorname{GlobalMax}([\boldsymbol{h}_{1}, \dots, \boldsymbol{h}_{n}])</script><p>对所有时间步的输出进行平均或者使用 Pooling 操作。</p>
<p>AttentionPooling1D是一种值得尝试的方案。假设词向量序列为 $\boldsymbol{X} = \left [\boldsymbol{x}_{1},\boldsymbol{x}_{2},\cdots,\boldsymbol{x}_{n} \right]$​ ，有未归一化的权重为，</p>
<script type="math/tex; mode=display">
[\alpha_{1}, \dots, \alpha_{n}] = s(\boldsymbol{x})= \boldsymbol{w}^{\mathsf{T}}\tanh(\boldsymbol{WX})</script><p>$\boldsymbol{W}, \boldsymbol{w}$​都是可学习的参数。在softmax归一化后有，</p>
<script type="math/tex; mode=display">
(\lambda_1, \dots, \lambda_n) = \operatorname{softmax}(\boldsymbol{w}^{\mathsf{T}}\tanh(\boldsymbol{WX}))</script><p>因此有，</p>
<script type="math/tex; mode=display">
\begin{align}
\boldsymbol{X}_{\text{sentence}}
&= \operatorname{AttentionPooling1D}(\boldsymbol{X})  \\
&= \sum_{i=1}^{n} \lambda_{i}\boldsymbol{X}_{i} \\
&= \operatorname{softmax}\Big[\boldsymbol{w}^{\mathsf{T}}\tanh(\boldsymbol{WX})\Big] \boldsymbol{X}
\end{align}</script><p>AttentionPooling1D可以看做是一种注意力机制，只不过是被动注意力机制。它一个好的side effect是可以使用归一化权重$[\lambda_1, \dots, \lambda_n]$​作为词或字的重要性权重。这给NLU任务的解释性提供极大的便利。​</p>
<h2 id="特征编码器Encoder"><a href="#特征编码器Encoder" class="headerlink" title="特征编码器Encoder"></a>特征编码器Encoder</h2><p>从模型出发，深度学习解决文本分类的思路都是特征提取 + 分类器的形式，特征提取过程随着网络层的加深，特征也可以分为：</p>
<ul>
<li>底层特征</li>
<li>中层特征</li>
<li>高层特征</li>
</ul>
<p>特征提取的基础模型可能是：</p>
<ul>
<li>CNN</li>
<li>RNN</li>
<li>引入 Attention 辅助</li>
<li>Transformer</li>
</ul>
<p>此外还有其他各种花式的模型组合可能只停留在创意设计层面吧，工业应用很少。而这些特征提取器的设计可能要关注多个方面：</p>
<ul>
<li>词的上下文情况</li>
<li>序列位置</li>
<li>长距离</li>
<li>…</li>
</ul>
<p>对于长文分类来说，可以尝试使用编码器：膨胀卷积 + 门卷积的结合。</p>
<p>首先是使用CNN作为基本模型，如TextCNN，Yoon Kim 在论文 Convolutional Neural Networks for Sentence Classification 中提出的 TextCNN 是 word-level 的文本分类模型。对于长文本来说，往往要加深卷积层来学习长距离复杂模式。当然，这也会导致每个向量的上下文信息变丰富。</p>
<p>不过这里也有另外一种处理长文的方案，具体是<strong>膨胀卷积</strong>也称为空洞卷积，在卷积核中插入空洞以扩大感受野而又不增加参数。其卷积过程可以用下图描述，</p>
<p><img src="../images/dilated-causla-cnn-layers.png" alt></p>
<p>膨胀大小一般依次取$r=1, 2, 4, 8, 16, \dots$，通过层堆叠且每层按指数扩大膨胀率来获得长距离依赖。取 dilated rate 为 $2^{n}$​​，n 为层数。膨胀卷积适合处理长序列，如传感器的多维时间序列、长文本等等。像LSTM模型一样，可以给卷积一个Gate，控制信息的流通</p>
<script type="math/tex; mode=display">
\boldsymbol{Y} = \operatorname{Conv}_{1}(\boldsymbol{X}) \otimes \sigma(\operatorname{Conv}_2(\boldsymbol{X}))</script><p>注意上式是两个卷积，超参数一致但不共享权重。结合残差，很自然有</p>
<script type="math/tex; mode=display">
\boldsymbol{Y} =\boldsymbol{X} + \operatorname{Conv}_{1}(\boldsymbol{X}) \otimes \sigma(\operatorname{Conv}_2(\boldsymbol{X}))</script><p>不过要注意两者的维度要一致，如果不一致可以通过一定的形状变换技巧处理为一致。注意到，假如$\sigma(\operatorname{Conv}_2(\boldsymbol{X})) \rightarrow 0$上述结构依旧有信息往下游流动，确保信息没有丢失。因此残差结构不仅仅解决梯度消失问题。</p>
<p>以上我们梳理了常见的模型在文本分类中的思路，其实，从整个流程看，选择什么模型并不是最关键，实践中遇到的问题才是最关键。不过总得有个 baseline 吧，如果没有更多的前置条件，我推荐使用 CNN，够快，单位时间内可以做更多的尝试。可能做最后为提高指标走火入魔引入了 ensemble，这时需要拉自己一把，从 bad case 中找问题。</p>
<h2 id="Embedding与词汇信息融入（更新）"><a href="#Embedding与词汇信息融入（更新）" class="headerlink" title="Embedding与词汇信息融入（更新）"></a>Embedding与词汇信息融入（更新）</h2><p>从Embedding角度：</p>
<ol>
<li>字Embedding</li>
<li>词Embedding</li>
<li>字词混合Embedding</li>
<li>PositionEmbedding（绝对位置编码、SinCosEmbedding、ComplexEmbedding）</li>
<li>RegionEmbedding</li>
<li>GlyphEmbedding（使用字形信息）</li>
<li>char-ngrams</li>
</ol>
<p>融入bi-ngrams，如下叠加，</p>
<p><img src="../images/hybrid-bi-ngrams-demo-1.png" alt></p>
<p>或者直接添加到tokens序列后面。类似的做法是fastText。</p>
<p>融入word-level信息，</p>
<p><img src="../images/hybrid-embedding-1.png" alt></p>
<p>以得到词中有字，字中有词。</p>
<h2 id="Dropout的运用"><a href="#Dropout的运用" class="headerlink" title="Dropout的运用"></a>Dropout的运用</h2><p>深度学习模型随着深度的增加，提取的特征也越丰富，但往往伴随着<strong>过度参数化</strong>，容易导致过拟合，因此引入正则化手段来缓解过拟合提生模型的泛化能力相对重要。Dropout是一种常用的手段。</p>
<p>Dropout在网络的训练阶段，随机丢弃部分神经元的输出，可以达到避免过拟合的目的。具体做法如下</p>
<script type="math/tex; mode=display">
\operatorname{dropout}(\boldsymbol{x}) = 
\begin{cases} 
\boldsymbol{m} \odot \boldsymbol{x} &\quad \text{on training} \\
p \boldsymbol{x} & \quad \text{on predict}
\end{cases}</script><p>其中m采样自伯努利分布，</p>
<script type="math/tex; mode=display">
\boldsymbol{m} \sim p^{z}(1-p)^{1-z}</script><p>dropout 可以看做是有意引入的噪声，加大模型的学习难度，提高模型的鲁棒性。此外，dropout 有集成学习的解释，是同一样本多次预测的平均。在计算上比常规意义上的集成学习更节省计算资源。所以，在很多任务中，dropout 是值得尝试的方法。基于Numpy的Python实现如下，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, p=<span class="number">0.5</span>, noise_shape=None, training=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x * p</span><br><span class="line">    <span class="keyword">if</span> noise_shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        noise_shape = np.shape(x)</span><br><span class="line">    mask = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-p, noise_shape)</span><br><span class="line">    <span class="keyword">return</span> p * mask</span><br></pre></td></tr></table></figure>
<p>mask也可以从均匀分布中生成，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, p=<span class="number">0.5</span>, noise_shape=None, training=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x * p</span><br><span class="line">    <span class="keyword">if</span> noise_shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        noise_shape = np.shape(x)</span><br><span class="line">    mask = np.uniform(<span class="number">0</span>, <span class="number">1</span>, noise_shape)</span><br><span class="line">    mask = mask &gt; (<span class="number">1</span> - p)</span><br><span class="line">    mask = np.cast(mask, np.float32)</span><br><span class="line">    <span class="keyword">return</span> p * mask</span><br></pre></td></tr></table></figure>
<p>以上，Dropout可以理解成一种集成策略，训练阶段Dropout会随机丢弃一定比例的信息，为模型的学习增添难度，这样模型就只能在剩下的信息上学习。由于每次丢弃的信息都是随机的，这样就有效避免模型只学习局部信息进而导致过拟合。在预测阶段，不进行丢弃，而是整合所有的信息。</p>
<p>Dropout一般接在word embedding层后、pooling层后、FC层（全联接层）后。在做baseline时，Embedding后先不要添加dropout，因为你根本就不知道dropout是否会其负作用。如果从一开始就加dropout，那就是默认其起到正作用。注意noise_shape的运用也是很重要的。</p>
<p>类似Dropout的操作就是在Embedding层前随机drop或替换token序列中的若干token。</p>
<h2 id="随机替换tokens"><a href="#随机替换tokens" class="headerlink" title="随机替换tokens"></a>随机替换tokens</h2><p>类似于Dropout，对输入的token序列中的若干元素进行随机替换为其他token，也可以当做是一种数据增强手段。效率高的做法是像Dropout一样作为层，而不是在pipeline上写，当然在pipeline写可以使用更灵活的替换逻辑。</p>
<p>然后，这一层放在Input之后，Embedding层之前。</p>
<h2 id="LayerNormalization"><a href="#LayerNormalization" class="headerlink" title="LayerNormalization"></a>LayerNormalization</h2><p>LayerNormalization对于模型的性能和收敛速度的影响很关键。LayerNormalization在Embedding层后，加速收敛，结果更稳定。对于NLP任务，BatchNormalization慎用应该慎用，有时候会起到负作用。</p>
<p>LayerNormalization一般可以加在Embedding后，Pooling后，对于分类任务，在softmax前也可以加LayerNormalization，这些操作都可以加快模型收敛。</p>
<h2 id="作为匹配问题"><a href="#作为匹配问题" class="headerlink" title="作为匹配问题"></a>作为匹配问题</h2><p>分类问题，尤其是多分类问题，可以看做是文本与标签匹配的问题。然后可以<strong>在标签上做数据增强或丰富标签的信息</strong>。假设训练好匹配模型$D$，那么类别为，</p>
<script type="math/tex; mode=display">
\hat{C} = \arg \max_i D(\boldsymbol{x}, C_i)</script><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>如果编码器是使用CNN类模型，那么可以通过引入位置编码加强CNN模型的位置感。一般使用绝对位置编码即可。假设有词向量序列，其第$i$个为$\boldsymbol{x}_i$，绝对位置编码为一个依赖$i$的函数，其添加所对应位置的词向量上，即</p>
<script type="math/tex; mode=display">
\widetilde{\boldsymbol{x}}_{i} = \boldsymbol{x}_{i} + \boldsymbol{p}_{i}</script><p>直观点看，</p>
<p><img src="../images/position-embedding-demom-1.png" alt></p>
<p>印象中最早是在Facebook的论文<a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a>中提出，这种位置编码只能支持有限长度的文本，因为训练Embedding是需要指定input_dim，那么可支持文本的最长长度就固定下来了。</p>
<h2 id="分词-or-不分词？"><a href="#分词-or-不分词？" class="headerlink" title="分词 or 不分词？"></a>分词 or 不分词？</h2><p>分词，对于中文场景来说，是一件让人头痛的事情，往往会引入噪声，在深度学习中可以不分词。一般使用单字，双字比单字大两个数量级。对于单纯的字来说，引入词能够带来词汇信息，缓解字Embedding表达能力不足的问题。对于单纯的词来说，引入字可以缓解分词引入的噪声以及词Embedding面临的OOV问题。</p>
<p>如果是要引入预训练权重，如word2vec词向量，那么当然要分词，不过要注意分词算法应该是该预训练权重对应的词汇表的分词算法一致。</p>
<p>三种方案：</p>
<ul>
<li>char level Embedding</li>
<li>word level Embedding</li>
<li>char level Embedding与word level Embedding混合使用</li>
</ul>
<p>分词的好处：</p>
<ul>
<li>分词可以引入预先训练好的向量，但是需要注意注意分词准确性与词向量是否匹配</li>
<li>word level embedding可以补充char level embedding表达力不足的问题</li>
</ul>
<p>字词混合好处：</p>
<ul>
<li>字词混合我的理解是平摊分词带来的噪声的引入</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>char level Embedding</td>
<td>/</td>
<td>/</td>
</tr>
<tr>
<td>word level Embedding</td>
<td>分词可以引入预先训练好的向量，但是需要注意注意分词准确性与词向量是否匹配；word level embedding可以补充char level embedding表达力不足的问题</td>
<td>不好的分词引入噪声；Embedding较char level Embedding大</td>
</tr>
<tr>
<td>char level Embedding与word level Embedding混合使用</td>
<td>字词混合我的理解是平摊分词带来的噪声的引入</td>
<td>Embedding更大</td>
</tr>
</tbody>
</table>
</div>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此外，还有很多Tricks，如权重平滑、数据增强、提前停止（EarlyStopping）、引入Loss（Focal Loss、Triplet Loss等等）、对抗学习、多任务学习。</p>
<p>除了以上Tricks，深度学习调参路漫漫，还需考虑优化器、参数初始化、学习率调整、批量大小、激活函数、数据预处理与扩充、正则化，这些都是Tricks。总之就是多实践多尝试吧。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></p>
<p>[2] Text Classification Algorithms: A Survey</p>
<p>[3] nndl.github.io</p>
<p>[4] <a href="https://www.aclweb.org/anthology/C14-1008/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/C14-1008/</a></p>
<p>[5] <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></p>
<p>[6] 《集体智慧编程》</p>
<p>[7] <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Multinomial_logistic_regression</a></p>
<p>转载请包括本文地址：<a href="../8487">https://allenwind.github.io/blog/8487</a><br>更多文章请参考：<a href="../archives/">https://allenwind.github.io/blog/archives/</a></p>
</div><div class="tags"><a href="/blog/tags/文本分类/">文本分类</a></div><div class="post-nav"><a class="pre" href="/blog/8750/">序列标注之NER、CWS经典模型HMM实现</a><a class="next" href="/blog/8450/">词向量系列（2）：VSM和主题模型思路</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://allenwind.github.io/blog"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/C-C/">C/C++</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据结构和算法/">数据结构和算法</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/记录/">记录</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/算法/" style="font-size: 15px;">算法</a> <a href="/blog/tags/C/" style="font-size: 15px;">C</a> <a href="/blog/tags/Python/" style="font-size: 15px;">Python</a> <a href="/blog/tags/设计模式/" style="font-size: 15px;">设计模式</a> <a href="/blog/tags/C语言/" style="font-size: 15px;">C语言</a> <a href="/blog/tags/数据库/" style="font-size: 15px;">数据库</a> <a href="/blog/tags/区块链/" style="font-size: 15px;">区块链</a> <a href="/blog/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/blog/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/blog/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/blog/tags/sudo/" style="font-size: 15px;">sudo</a> <a href="/blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/blog/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/blog/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/blog/tags/Git/" style="font-size: 15px;">Git</a> <a href="/blog/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/blog/tags/git/" style="font-size: 15px;">git</a> <a href="/blog/tags/命令行/" style="font-size: 15px;">命令行</a> <a href="/blog/tags/gcc/" style="font-size: 15px;">gcc</a> <a href="/blog/tags/Systemd/" style="font-size: 15px;">Systemd</a> <a href="/blog/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/blog/tags/网络/" style="font-size: 15px;">网络</a> <a href="/blog/tags/操作系统/" style="font-size: 15px;">操作系统</a> <a href="/blog/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/blog/tags/记录/" style="font-size: 15px;">记录</a> <a href="/blog/tags/数据结构/" style="font-size: 15px;">数据结构</a> <a href="/blog/tags/Go/" style="font-size: 15px;">Go</a> <a href="/blog/tags/go/" style="font-size: 15px;">go</a> <a href="/blog/tags/Java/" style="font-size: 15px;">Java</a> <a href="/blog/tags/LRU/" style="font-size: 15px;">LRU</a> <a href="/blog/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/blog/tags/搜索引擎/" style="font-size: 15px;">搜索引擎</a> <a href="/blog/tags/剑指Offer/" style="font-size: 15px;">剑指Offer</a> <a href="/blog/tags/函数式/" style="font-size: 15px;">函数式</a> <a href="/blog/tags/Stack/" style="font-size: 15px;">Stack</a> <a href="/blog/tags/并发编程/" style="font-size: 15px;">并发编程</a> <a href="/blog/tags/网络编程/" style="font-size: 15px;">网络编程</a> <a href="/blog/tags/并发/" style="font-size: 15px;">并发</a> <a href="/blog/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/blog/tags/装饰器/" style="font-size: 15px;">装饰器</a> <a href="/blog/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/blog/tags/数学/" style="font-size: 15px;">数学</a> <a href="/blog/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/blog/tags/投资/" style="font-size: 15px;">投资</a> <a href="/blog/tags/度量/" style="font-size: 15px;">度量</a> <a href="/blog/tags/概率/" style="font-size: 15px;">概率</a> <a href="/blog/tags/统计/" style="font-size: 15px;">统计</a> <a href="/blog/tags/信息论/" style="font-size: 15px;">信息论</a> <a href="/blog/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/blog/tags/贝叶斯/" style="font-size: 15px;">贝叶斯</a> <a href="/blog/tags/最小二乘法/" style="font-size: 15px;">最小二乘法</a> <a href="/blog/tags/不等式/" style="font-size: 15px;">不等式</a> <a href="/blog/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/blog/tags/变分推断/" style="font-size: 15px;">变分推断</a> <a href="/blog/tags/随机/" style="font-size: 15px;">随机</a> <a href="/blog/tags/采样/" style="font-size: 15px;">采样</a> <a href="/blog/tags/逼近/" style="font-size: 15px;">逼近</a> <a href="/blog/tags/光滑/" style="font-size: 15px;">光滑</a> <a href="/blog/tags/分布/" style="font-size: 15px;">分布</a> <a href="/blog/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/blog/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/blog/tags/词向量/" style="font-size: 15px;">词向量</a> <a href="/blog/tags/信息/" style="font-size: 15px;">信息</a> <a href="/blog/tags/交叉验证/" style="font-size: 15px;">交叉验证</a> <a href="/blog/tags/分类/" style="font-size: 15px;">分类</a> <a href="/blog/tags/指标/" style="font-size: 15px;">指标</a> <a href="/blog/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/blog/tags/证明/" style="font-size: 15px;">证明</a> <a href="/blog/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/blog/tags/损失函数/" style="font-size: 15px;">损失函数</a> <a href="/blog/tags/回归/" style="font-size: 15px;">回归</a> <a href="/blog/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/blog/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/blog/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/blog/tags/Transformer/" style="font-size: 15px;">Transformer</a> <a href="/blog/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/blog/tags/对抗训练/" style="font-size: 15px;">对抗训练</a> <a href="/blog/tags/文本分类/" style="font-size: 15px;">文本分类</a> <a href="/blog/tags/分词/" style="font-size: 15px;">分词</a> <a href="/blog/tags/并行/" style="font-size: 15px;">并行</a> <a href="/blog/tags/NER/" style="font-size: 15px;">NER</a> <a href="/blog/tags/概率图/" style="font-size: 15px;">概率图</a> <a href="/blog/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/blog/tags/图/" style="font-size: 15px;">图</a> <a href="/blog/tags/梯度/" style="font-size: 15px;">梯度</a> <a href="/blog/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/blog/tags/注意力/" style="font-size: 15px;">注意力</a> <a href="/blog/tags/序列编码/" style="font-size: 15px;">序列编码</a> <a href="/blog/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/blog/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/blog/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/blog/tags/注意力机制/" style="font-size: 15px;">注意力机制</a> <a href="/blog/tags/磁盘故障/" style="font-size: 15px;">磁盘故障</a> <a href="/blog/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/blog/tags/卷积/" style="font-size: 15px;">卷积</a> <a href="/blog/tags/优化/" style="font-size: 15px;">优化</a> <a href="/blog/tags/调参/" style="font-size: 15px;">调参</a> <a href="/blog/tags/Flask/" style="font-size: 15px;">Flask</a> <a href="/blog/tags/web/" style="font-size: 15px;">web</a> <a href="/blog/tags/socket/" style="font-size: 15px;">socket</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/16228/">分析与拓展：Transformer中的MultiHeadAttention为什么使用scaled？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16113/">Tensorflow的多卡训练：原理和实践</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16051/">机器学习之分类问题的评估指标总结</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16003/">天马行空：设计自己的激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15887/">函数光滑近似（4）：Heaviside step函数及其应用</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15205/">引入参数控制softmax的smooth程度</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15110/">分析与拓展：多分类模型的输出为什么使用softmax？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14990/">一种基于光滑逼近的正态分布采样法</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14721/">GELU由来：从狄拉克函数到GELU激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14465/">Lp范数的上下界分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/allenwind" title="My GitHub" target="_blank">My GitHub</a><ul></ul><a href="http://www.arxiv-sanity.com/" title="arxiv-sanity" target="_blank">arxiv-sanity</a><ul></ul><a href="https://arxiv.org/" title="arxiv.org" target="_blank">arxiv.org</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/blog/." rel="nofollow">Mr.Feng Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>