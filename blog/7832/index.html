<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="你好，世界！"><title>深度学习中的参数初始化及其数学分析 | Mr.Feng Blog</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深度学习中的参数初始化及其数学分析</h1><a id="logo" href="/blog/.">Mr.Feng Blog</a><p class="description">NLP、深度学习、机器学习、Python、Go</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">深度学习中的参数初始化及其数学分析</h1><div class="post-meta">Nov 20, 2018<span> | </span><span class="category"><a href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#常数初始化"><span class="toc-number">1.</span> <span class="toc-text">常数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机初始化"><span class="toc-number">2.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预训练初始化"><span class="toc-number">3.</span> <span class="toc-text">预训练初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#方差缩放初始化"><span class="toc-number">4.</span> <span class="toc-text">方差缩放初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正交初始化"><span class="toc-number">5.</span> <span class="toc-text">正交初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><p>在深度学习中，模型的权重可以看作是随机向量或随机矩阵，在模型训练前往往需要参数初始化。参数初始化方法有很多，包括固定值初始化、随机初始化、预训练初始化。对于大型模型来说，好的初始化能够帮助模型收敛；差的初始化可能让模型效果变差，甚至根本无法收敛。</p>
<a id="more"></a>
<p>本文讨论深度学习中模型参数的初始化，其中<strong>随机向量与随机矩阵及其在深度学习中的讨论</strong>，见历史文章<a href="../7828">浅谈随机向量与随机矩阵</a>。</p>
<h2 id="常数初始化"><a href="#常数初始化" class="headerlink" title="常数初始化"></a>常数初始化</h2><p>某些特殊的参数全部取0或者1，不过也需要考虑例外：</p>
<ul>
<li>如无特殊情况Dense中的偏置bias一般都是0，注意把bais初始化为0并不等于不设置bias。对于Dense，bias的存在往往很重要</li>
<li>对于使用ReLU的神经元， 可以将偏置bias设为$\alpha=0.01$， 使得训练初期更容易激活， 从而获得一定的梯度来进行误差反向传播</li>
<li>在LSTM网络的遗忘门中， 偏置bias通常初始化为1遗忘门或2， 使得时序上的梯度变大</li>
</ul>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>参数初始化的基本思想是，对于一个模型层$f(x)$来说，输出</p>
<script type="math/tex; mode=display">
y = f(x)</script><p>输入$x$​和输出$y$​的方差和均值保持不变。保持统计特征的不便有利于模型的稳定与收敛。</p>
<p>随机初始化（Random Initialization）让参数从某个概率分布总采样。随机初始化就更好理解了，既然不知道什么值更好，那就乱取一通好了。这里的关键是从什么分布中采样。常见的初始化分布有：</p>
<ul>
<li>正太分布</li>
<li>截断正太分布</li>
<li>均匀分布</li>
</ul>
<p>正太分布和均匀分布的采样很好理解，而截断正太分布是考虑样本满足给定的均值、方差且在一定的区间内$x \in [a, b]$​，概率密度表示，</p>
<script type="math/tex; mode=display">
f(x;\mu ,\sigma ,a,b)={\frac {1}{\sigma }}\,{\frac {\phi ({\frac {x-\mu }{\sigma }})}{\Phi ({\frac {b-\mu }{\sigma }})-\Phi ({\frac {a-\mu }{\sigma }})}}</script><p>通常是0中心，即均值为零，取值区间为，</p>
<script type="math/tex; mode=display">
- 2 \sigma \lt x_{i} \lt 2 \sigma</script><p>由于正太分布的取值区间是整个实数域，在极端情况下会采样到非常大的值，例如以标准差为1，从正太分布中采样1000000000个数，最大值可以达到6，远大于$3\sigma$。因此使用正太分布初始化，一般都使用截断正太分布。<strong>本文接下来提到的正太分布均指截断正太分布</strong>。</p>
<p>那么截断的正太分布初始化和均匀分布初始化如何选择？我认为和其他网络组建保持一致即可。</p>
<h2 id="预训练初始化"><a href="#预训练初始化" class="headerlink" title="预训练初始化"></a>预训练初始化</h2><p>这是CV中常用的初始化方式。模型训练前使用在大规模数据上训练过的模型所获得的好的参数作为初始值。随后在当前模型进行fine-tuning。<strong>预训练参数初始化可以起到正则化作用</strong>，相对与随机初始化，前者在训练时能够在更小的参数空间中搜索最优解。</p>
<p>预训练模型在从头训练是也需要初始化，常见方法是方差缩放初始化。</p>
<h2 id="方差缩放初始化"><a href="#方差缩放初始化" class="headerlink" title="方差缩放初始化"></a>方差缩放初始化</h2><p>方差缩放初始化方法通过对方差缩放，以达到层的输出、输出信号的方差和均值保存不变。</p>
<p>对于正太分布，方差缩放初始化每个参数采样自，</p>
<script type="math/tex; mode=display">
x_i \sim N(0, \frac{d}{n})</script><p>$n$的取值有三种，分别是输入维度、输出维度、输入维度与输出维度的平均。$d \gt 0$是缩放因子，用来确定正太分布的方差$\sigma^2$。</p>
<p>最常用的初始化方法GlorotNormal，$n$值使用输入维度与输出维度的平均，$d=2$，</p>
<script type="math/tex; mode=display">
x_i \sim N(0, \frac{2}{d_i + d_o})</script><p>初始化方法HeNormal，</p>
<script type="math/tex; mode=display">
x_i \sim N(0, \frac{2}{d_i})</script><p>这里注意到relu激活函数会把一半输出置零。</p>
<p>初始化方法LecunNormal，取$d=1$，</p>
<script type="math/tex; mode=display">
x_i \sim N(0, \frac{1}{d_i})</script><p>对于均匀分布，方差缩放初始化每个参数采样自，</p>
<script type="math/tex; mode=display">
x_i \sim U(-\sqrt{\frac{3d}{n}}, \sqrt{\frac{3d}{n}})</script><p>这里主要是考虑，采样自该分布，方差保持不变，</p>
<script type="math/tex; mode=display">
\frac{(b-a)^2}{2} = \frac{\big(\sqrt{\frac{3d}{n}} - (-\sqrt{\frac{3d}{n}})\big)^2}{12} = \frac{d}{n}</script><p>同上，GlorotUniform、HeUniform、LecunUniform只是在$d,n$上的取值有差异。</p>
<h2 id="正交初始化"><a href="#正交初始化" class="headerlink" title="正交初始化"></a>正交初始化</h2><p>正交初始化要求参数矩阵具有正交性，即</p>
<script type="math/tex; mode=display">
W^{\top}W = I</script><p>于是有，</p>
<script type="math/tex; mode=display">
\|Wx\|^{2} = (Wx)^{\top}(Wx) = x^{\top}(W^{\top}W)x = X^{\top}I x = \|x\|^2</script><p>也就是说，在正交矩阵的线下变换下，$x$具有模长不变性。这样一来，在反向传播中，</p>
<script type="math/tex; mode=display">
\delta^{(l-1)} = (W^{(l)})^{\top} \delta^{(l)}</script><p>涉及到矩阵$W^{(l)}$的连续乘积就会近似于单位矩阵，这就解决了梯度爆炸或梯度消失问题。</p>
<p>获得正交矩阵的方法：</p>
<ol>
<li>独立采样$x_{ij} \sim N(0, 1)$​获得矩阵$W$</li>
<li>对矩阵$W$进行SVD分解，$W = U\Sigma V^{-1}$​​</li>
<li>获得两个正交矩阵，使用其中一个作为参数权重</li>
</ol>
<p>另外还有一种方法在<a href="../7828">浅谈随机向量与随机矩阵</a>提到，从向量可以推广到矩阵，假设矩阵$W_{n \times n} = [a_{ij}]$​，如果每个元素均采样自正太分布，</p>
<script type="math/tex; mode=display">
w_{ij} \sim N(0, \frac{1}{n})</script><p>那么，</p>
<script type="math/tex; mode=display">
W W^{\top} \approx I</script><p>也就是说满足一定采样条件的随机矩阵几乎正交。事实上，对于$W \in R^{m \times n} , m \ge n$​的矩阵也成立。也就是说，对于某一层，</p>
<script type="math/tex; mode=display">
y = W \times x</script><p>$x$的输入时$n$维，输出是$m$维，且$m \ge n$，是一个生维过程且保持模长不变。</p>
<p>QR分解有称为<strong>正交三角分解</strong>也是获得正交矩阵的方法之一，假设有列向量线性无关的矩阵A，那么可分解成，</p>
<script type="math/tex; mode=display">
A_{m \times n} = Q \times R</script><p>$Q \in R^{m \times n}$是正交矩阵，即$Q \times Q^{\top} = I$，$R \in R^{n \times n}$​是上三角矩阵。</p>
<p>因此正交矩阵的获取方法：</p>
<ul>
<li>矩阵的SVD分解</li>
<li>矩阵的QR分解</li>
<li>以一定的规则随机初始化矩阵</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用更好的参数初始化方法，有利于SGD等方法的优化效率，最终会关系到模型的泛化能力。正所谓，先天足，后天强。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://en.wikipedia.org/wiki/Multivariate_random_variable" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Multivariate_random_variable</a></p>
<p>[2] <a href="https://en.wikipedia.iwiki.uk/wiki/Stochastic_matrix" target="_blank" rel="noopener">https://en.wikipedia.iwiki.uk/wiki/Stochastic_matrix</a></p>
<p>[3] <a href="https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test</a></p>
<p>[4] <a href="https://en.wikipedia.org/wiki/Truncated_normal_distribution" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Truncated_normal_distribution</a></p>
<p>转载请包括本文地址：<a href="../7832/">https://allenwind.github.io/blog/7832/</a><br>更多文章请参考：<a href="../archives/">https://allenwind.github.io/blog/archives/</a></p>
</div><div class="tags"><a href="/blog/tags/数学/">数学</a><a href="/blog/tags/深度学习/">深度学习</a><a href="/blog/tags/随机/">随机</a><a href="/blog/tags/矩阵/">矩阵</a><a href="/blog/tags/优化/">优化</a></div><div class="post-nav"><a class="pre" href="/blog/7890/">深入理解CNN及其网络架构设计</a><a class="next" href="/blog/7828/">浅谈随机向量与随机矩阵</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://allenwind.github.io/blog"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/C-C/">C/C++</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Go/">Go</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数学/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/数据结构和算法/">数据结构和算法</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/机器学习深度学习/">机器学习深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/记录/">记录</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/算法/" style="font-size: 15px;">算法</a> <a href="/blog/tags/C/" style="font-size: 15px;">C</a> <a href="/blog/tags/Python/" style="font-size: 15px;">Python</a> <a href="/blog/tags/设计模式/" style="font-size: 15px;">设计模式</a> <a href="/blog/tags/C语言/" style="font-size: 15px;">C语言</a> <a href="/blog/tags/数据库/" style="font-size: 15px;">数据库</a> <a href="/blog/tags/区块链/" style="font-size: 15px;">区块链</a> <a href="/blog/tags/ACID/" style="font-size: 15px;">ACID</a> <a href="/blog/tags/分布式/" style="font-size: 15px;">分布式</a> <a href="/blog/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/blog/tags/SQL/" style="font-size: 15px;">SQL</a> <a href="/blog/tags/NoSQL/" style="font-size: 15px;">NoSQL</a> <a href="/blog/tags/sudo/" style="font-size: 15px;">sudo</a> <a href="/blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/blog/tags/git/" style="font-size: 15px;">git</a> <a href="/blog/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/blog/tags/Git/" style="font-size: 15px;">Git</a> <a href="/blog/tags/gcc/" style="font-size: 15px;">gcc</a> <a href="/blog/tags/Systemd/" style="font-size: 15px;">Systemd</a> <a href="/blog/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/blog/tags/命令行/" style="font-size: 15px;">命令行</a> <a href="/blog/tags/网络/" style="font-size: 15px;">网络</a> <a href="/blog/tags/操作系统/" style="font-size: 15px;">操作系统</a> <a href="/blog/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/blog/tags/记录/" style="font-size: 15px;">记录</a> <a href="/blog/tags/数据结构/" style="font-size: 15px;">数据结构</a> <a href="/blog/tags/Go/" style="font-size: 15px;">Go</a> <a href="/blog/tags/go/" style="font-size: 15px;">go</a> <a href="/blog/tags/Java/" style="font-size: 15px;">Java</a> <a href="/blog/tags/LRU/" style="font-size: 15px;">LRU</a> <a href="/blog/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/blog/tags/搜索引擎/" style="font-size: 15px;">搜索引擎</a> <a href="/blog/tags/剑指Offer/" style="font-size: 15px;">剑指Offer</a> <a href="/blog/tags/函数式/" style="font-size: 15px;">函数式</a> <a href="/blog/tags/Stack/" style="font-size: 15px;">Stack</a> <a href="/blog/tags/并发编程/" style="font-size: 15px;">并发编程</a> <a href="/blog/tags/网络编程/" style="font-size: 15px;">网络编程</a> <a href="/blog/tags/并发/" style="font-size: 15px;">并发</a> <a href="/blog/tags/HTTP/" style="font-size: 15px;">HTTP</a> <a href="/blog/tags/装饰器/" style="font-size: 15px;">装饰器</a> <a href="/blog/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/blog/tags/数学/" style="font-size: 15px;">数学</a> <a href="/blog/tags/时间序列/" style="font-size: 15px;">时间序列</a> <a href="/blog/tags/投资/" style="font-size: 15px;">投资</a> <a href="/blog/tags/概率/" style="font-size: 15px;">概率</a> <a href="/blog/tags/统计/" style="font-size: 15px;">统计</a> <a href="/blog/tags/信息论/" style="font-size: 15px;">信息论</a> <a href="/blog/tags/度量/" style="font-size: 15px;">度量</a> <a href="/blog/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/blog/tags/贝叶斯/" style="font-size: 15px;">贝叶斯</a> <a href="/blog/tags/最小二乘法/" style="font-size: 15px;">最小二乘法</a> <a href="/blog/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/blog/tags/变分推断/" style="font-size: 15px;">变分推断</a> <a href="/blog/tags/随机/" style="font-size: 15px;">随机</a> <a href="/blog/tags/不等式/" style="font-size: 15px;">不等式</a> <a href="/blog/tags/采样/" style="font-size: 15px;">采样</a> <a href="/blog/tags/光滑/" style="font-size: 15px;">光滑</a> <a href="/blog/tags/逼近/" style="font-size: 15px;">逼近</a> <a href="/blog/tags/分布/" style="font-size: 15px;">分布</a> <a href="/blog/tags/特征工程/" style="font-size: 15px;">特征工程</a> <a href="/blog/tags/集成学习/" style="font-size: 15px;">集成学习</a> <a href="/blog/tags/分类/" style="font-size: 15px;">分类</a> <a href="/blog/tags/证明/" style="font-size: 15px;">证明</a> <a href="/blog/tags/交叉验证/" style="font-size: 15px;">交叉验证</a> <a href="/blog/tags/词向量/" style="font-size: 15px;">词向量</a> <a href="/blog/tags/信息/" style="font-size: 15px;">信息</a> <a href="/blog/tags/损失函数/" style="font-size: 15px;">损失函数</a> <a href="/blog/tags/回归/" style="font-size: 15px;">回归</a> <a href="/blog/tags/指标/" style="font-size: 15px;">指标</a> <a href="/blog/tags/正则化/" style="font-size: 15px;">正则化</a> <a href="/blog/tags/矩阵/" style="font-size: 15px;">矩阵</a> <a href="/blog/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/blog/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/blog/tags/Embedding/" style="font-size: 15px;">Embedding</a> <a href="/blog/tags/Transformer/" style="font-size: 15px;">Transformer</a> <a href="/blog/tags/Attention/" style="font-size: 15px;">Attention</a> <a href="/blog/tags/对抗训练/" style="font-size: 15px;">对抗训练</a> <a href="/blog/tags/文本分类/" style="font-size: 15px;">文本分类</a> <a href="/blog/tags/分词/" style="font-size: 15px;">分词</a> <a href="/blog/tags/并行/" style="font-size: 15px;">并行</a> <a href="/blog/tags/NER/" style="font-size: 15px;">NER</a> <a href="/blog/tags/HMM/" style="font-size: 15px;">HMM</a> <a href="/blog/tags/概率图/" style="font-size: 15px;">概率图</a> <a href="/blog/tags/图/" style="font-size: 15px;">图</a> <a href="/blog/tags/可视化/" style="font-size: 15px;">可视化</a> <a href="/blog/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/blog/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/blog/tags/梯度/" style="font-size: 15px;">梯度</a> <a href="/blog/tags/注意力/" style="font-size: 15px;">注意力</a> <a href="/blog/tags/序列编码/" style="font-size: 15px;">序列编码</a> <a href="/blog/tags/磁盘故障/" style="font-size: 15px;">磁盘故障</a> <a href="/blog/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/blog/tags/卷积/" style="font-size: 15px;">卷积</a> <a href="/blog/tags/优化/" style="font-size: 15px;">优化</a> <a href="/blog/tags/BERT/" style="font-size: 15px;">BERT</a> <a href="/blog/tags/调参/" style="font-size: 15px;">调参</a> <a href="/blog/tags/Flask/" style="font-size: 15px;">Flask</a> <a href="/blog/tags/web/" style="font-size: 15px;">web</a> <a href="/blog/tags/socket/" style="font-size: 15px;">socket</a> <a href="/blog/tags/安全/" style="font-size: 15px;">安全</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/16113/">Tensorflow的多卡训练：原理和实践</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16051/">机器学习之分类问题的评估指标总结</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/16003/">天马行空：设计自己的激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15887/">函数光滑近似（4）：Heaviside step函数及其应用</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15205/">引入参数控制softmax的smooth程度</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/15110/">分析与拓展：多分类模型的输出为什么使用softmax？</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14990/">一种基于光滑逼近的正态分布采样法</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14721/">GELU由来：从狄拉克函数到GELU激活函数</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14465/">Lp范数的上下界分析</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/14441/">logsumexp函数分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/allenwind" title="My GitHub" target="_blank">My GitHub</a><ul></ul><a href="http://www.arxiv-sanity.com/" title="arxiv-sanity" target="_blank">arxiv-sanity</a><ul></ul><a href="https://arxiv.org/" title="arxiv.org" target="_blank">arxiv.org</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/blog/." rel="nofollow">Mr.Feng Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>